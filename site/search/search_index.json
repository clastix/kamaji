{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"architecture/","text":"Architecture","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"concepts/","text":"Concepts Kamaji is a Kubernetes Operator. It turns any Kubernetes cluster into an \u201cadmin cluster\u201d to orchestrate other Kubernetes clusters called \u201ctenant clusters\u201d . These are requirements of the design behind Kamaji: Communication between the \u201cadmin cluster\u201d and a \u201ctenant cluster\u201d is unidirectional. The \u201cadmin cluster\u201d manages a \u201ctenant cluster\u201d , but a \u201ctenant cluster\u201d has no awareness of the \u201cadmin cluster\u201d . Communication between different \u201ctenant clusters\u201d is not allowed. The worker nodes of tenant should not run anything beyond tenant's workloads. Goals and scope may vary as the project evolves. Tenant Control Plane What makes Kamaji special is that the Control Plane of a \u201ctenant cluster\u201d is just one or more regular pods running in a namespace of the \u201cadmin cluster\u201d instead of a dedicated set of Virtual Machines. This solution makes running control planes at scale cheaper and easier to deploy and operate. The Tenant Control Plane components are packaged in the same way they are running in bare metal or virtual nodes. We leverage the kubeadm code to set up the control plane components as they were running on their own server. The unchanged images of upstream kube-apiserver , kube-scheduler , and kube-controller-manager are used. High Availability and rolling updates of the Tenant Control Plane pods are provided by a regular Deployment. Autoscaling based on the metrics is available. A Service is used to espose the Tenant Control Plane outside of the \u201cadmin cluster\u201d . The LoadBalancer service type is used, NodePort and ClusterIP with an Ingress Controller are still viable options, depending on the case. Kamaji offers a Custom Resource Definition to provide a declarative approach of managing a Tenant Control Plane. This CRD is called TenantControlPlane , or tcp in short. All the \u201ctenant clusters\u201d built with Kamaji are fully compliant CNCF Kubernetes clusters and are compatible with the standard Kubernetes toolchains everybody knows and loves. See CNCF compliance . Tenant worker nodes And what about the tenant worker nodes? They are just \"worker nodes\" , i.e. regular virtual or bare metal machines, connecting to the APIs server of the Tenant Control Plane. Kamaji's goal is to manage the lifecycle of hundreds of these \u201ctenant clusters\u201d , not only one, so how to add another tenant cluster to Kamaji? As you could expect, you have just deploys a new Tenant Control Plane in one of the \u201cadmin cluster\u201d namespace, and then joins the tenant worker nodes to it. We have in roadmap, the Cluster APIs support as well as a Terraform provider so that you can create \u201ctenant clusters\u201d in a declarative way. Datastores Putting the Tenant Control Plane in a pod is the easiest part. Also, we have to make sure each tenant cluster saves the state to be able to store and retrieve data. A dedicated etcd cluster for each tenant cluster doesn\u2019t scale well for a managed service because etcd data persistence can be cumbersome at scale, rising the operational effort to mitigate it. So we have to find an alternative keeping in mind our goal for a resilient and cost-optimized solution at the same time. As we can deploy any Kubernetes cluster with an external etcd cluster, we explored this option for the tenant control planes. On the admin cluster, we can deploy a multi-tenant etcd datastore to save the state of multiple tenant clusters. Kamaji offers a Custom Resource Definition called DataStore to provide a declarative approach of managing Tenant datastores. With this solution, the resiliency is guaranteed by the usual etcd mechanism, and the pods' count remains under control, so it solves the main goal of resiliency and costs optimization. The trade-off here is that we have to operate an external datastore, in addition to etcd of the \u201cadmin cluster\u201d and manage the access to be sure that each \u201ctenant cluster\u201d uses only its data. Other storage drivers Kamaji offers the option of using a more capable datastore than etcd to save the state of multiple tenants' clusters. Thanks to the native kine integration, you can run MySQL or PostgreSQL compatible databases as datastore for \u201ctenant clusters\u201d . Pooling By default, Kamaji is expecting to persist all the \u201ctenant clusters\u201d data in a unique datastore that could be backed by different drivers. However, you can pick a different datastore for a specific set of \u201ctenant clusters\u201d that could have different resources assigned or a different tiering. Pooling of multiple datastore is an option you can leverage for a very large set of \u201ctenant clusters\u201d so you can distribute the load properly. As future improvements, we have a datastore scheduler feature in roadmap so that Kamaji itself can assign automatically a \u201ctenant cluster\u201d to the best datastore in the pool. Konnectivity In addition to the standard control plane containers, Kamaji creates an instance of konnectivity-server running as sidecar container in the tcp pod and exposed on port 8132 of the tcp service. This is required when the tenant worker nodes are not reachable from the tcp pods. The Konnectivity service consists of two parts: the Konnectivity server in the tenant control plane pod and the Konnectivity agents running on the tenant worker nodes. After worker nodes joined the tenant control plane, the Konnectivity agents initiate connections to the Konnectivity server and maintain the network connections. After enabling the Konnectivity service, all control plane to worker nodes traffic goes through these connections. In Kamaji, Konnectivity is enabled by default and can be disabled when not required.","title":"Concepts"},{"location":"concepts/#concepts","text":"Kamaji is a Kubernetes Operator. It turns any Kubernetes cluster into an \u201cadmin cluster\u201d to orchestrate other Kubernetes clusters called \u201ctenant clusters\u201d . These are requirements of the design behind Kamaji: Communication between the \u201cadmin cluster\u201d and a \u201ctenant cluster\u201d is unidirectional. The \u201cadmin cluster\u201d manages a \u201ctenant cluster\u201d , but a \u201ctenant cluster\u201d has no awareness of the \u201cadmin cluster\u201d . Communication between different \u201ctenant clusters\u201d is not allowed. The worker nodes of tenant should not run anything beyond tenant's workloads. Goals and scope may vary as the project evolves.","title":"Concepts"},{"location":"concepts/#tenant-control-plane","text":"What makes Kamaji special is that the Control Plane of a \u201ctenant cluster\u201d is just one or more regular pods running in a namespace of the \u201cadmin cluster\u201d instead of a dedicated set of Virtual Machines. This solution makes running control planes at scale cheaper and easier to deploy and operate. The Tenant Control Plane components are packaged in the same way they are running in bare metal or virtual nodes. We leverage the kubeadm code to set up the control plane components as they were running on their own server. The unchanged images of upstream kube-apiserver , kube-scheduler , and kube-controller-manager are used. High Availability and rolling updates of the Tenant Control Plane pods are provided by a regular Deployment. Autoscaling based on the metrics is available. A Service is used to espose the Tenant Control Plane outside of the \u201cadmin cluster\u201d . The LoadBalancer service type is used, NodePort and ClusterIP with an Ingress Controller are still viable options, depending on the case. Kamaji offers a Custom Resource Definition to provide a declarative approach of managing a Tenant Control Plane. This CRD is called TenantControlPlane , or tcp in short. All the \u201ctenant clusters\u201d built with Kamaji are fully compliant CNCF Kubernetes clusters and are compatible with the standard Kubernetes toolchains everybody knows and loves. See CNCF compliance .","title":"Tenant Control Plane"},{"location":"concepts/#tenant-worker-nodes","text":"And what about the tenant worker nodes? They are just \"worker nodes\" , i.e. regular virtual or bare metal machines, connecting to the APIs server of the Tenant Control Plane. Kamaji's goal is to manage the lifecycle of hundreds of these \u201ctenant clusters\u201d , not only one, so how to add another tenant cluster to Kamaji? As you could expect, you have just deploys a new Tenant Control Plane in one of the \u201cadmin cluster\u201d namespace, and then joins the tenant worker nodes to it. We have in roadmap, the Cluster APIs support as well as a Terraform provider so that you can create \u201ctenant clusters\u201d in a declarative way.","title":"Tenant worker nodes"},{"location":"concepts/#datastores","text":"Putting the Tenant Control Plane in a pod is the easiest part. Also, we have to make sure each tenant cluster saves the state to be able to store and retrieve data. A dedicated etcd cluster for each tenant cluster doesn\u2019t scale well for a managed service because etcd data persistence can be cumbersome at scale, rising the operational effort to mitigate it. So we have to find an alternative keeping in mind our goal for a resilient and cost-optimized solution at the same time. As we can deploy any Kubernetes cluster with an external etcd cluster, we explored this option for the tenant control planes. On the admin cluster, we can deploy a multi-tenant etcd datastore to save the state of multiple tenant clusters. Kamaji offers a Custom Resource Definition called DataStore to provide a declarative approach of managing Tenant datastores. With this solution, the resiliency is guaranteed by the usual etcd mechanism, and the pods' count remains under control, so it solves the main goal of resiliency and costs optimization. The trade-off here is that we have to operate an external datastore, in addition to etcd of the \u201cadmin cluster\u201d and manage the access to be sure that each \u201ctenant cluster\u201d uses only its data.","title":"Datastores"},{"location":"concepts/#other-storage-drivers","text":"Kamaji offers the option of using a more capable datastore than etcd to save the state of multiple tenants' clusters. Thanks to the native kine integration, you can run MySQL or PostgreSQL compatible databases as datastore for \u201ctenant clusters\u201d .","title":"Other storage drivers"},{"location":"concepts/#pooling","text":"By default, Kamaji is expecting to persist all the \u201ctenant clusters\u201d data in a unique datastore that could be backed by different drivers. However, you can pick a different datastore for a specific set of \u201ctenant clusters\u201d that could have different resources assigned or a different tiering. Pooling of multiple datastore is an option you can leverage for a very large set of \u201ctenant clusters\u201d so you can distribute the load properly. As future improvements, we have a datastore scheduler feature in roadmap so that Kamaji itself can assign automatically a \u201ctenant cluster\u201d to the best datastore in the pool.","title":"Pooling"},{"location":"concepts/#konnectivity","text":"In addition to the standard control plane containers, Kamaji creates an instance of konnectivity-server running as sidecar container in the tcp pod and exposed on port 8132 of the tcp service. This is required when the tenant worker nodes are not reachable from the tcp pods. The Konnectivity service consists of two parts: the Konnectivity server in the tenant control plane pod and the Konnectivity agents running on the tenant worker nodes. After worker nodes joined the tenant control plane, the Konnectivity agents initiate connections to the Konnectivity server and maintain the network connections. After enabling the Konnectivity service, all control plane to worker nodes traffic goes through these connections. In Kamaji, Konnectivity is enabled by default and can be disabled when not required.","title":"Konnectivity"},{"location":"contribute/","text":"","title":"Contribute"},{"location":"getting-started/","text":"Getting started This document explains how to deploy a minimal Kamaji setup on KinD for development scopes. Please refer to the Kamaji documentation for understanding all the terms used in this guide, as for example: admin cluster , tenant cluster , and tenant control plane . Pre-requisites We assume you have installed on your workstation: Docker KinD kubectl@v1.25.0 kubeadm@v1.25.0 jq openssl cfssl cfssljson Starting from Kamaji v0.0.2, kubectl and kubeadm need to meet at least minimum version to v1.25.0 : this is required due to the latest changes addressed from the release Kubernetes 1.25 release regarding the kubelet-config ConfigMap required for the node join. Setup Kamaji on KinD The instance of Kamaji is made of a single node hosting: admin control-plane admin worker multi-tenant datastore Standard installation You can install your KinD cluster, ETCD multi-tenant cluster and Kamaji operator with a single command : $ make -C deploy/kind Now you can create your first TenantControlPlane . Data store-specific Kamaji offers the possibility of using a different storage system than ETCD for the tenants, like MySQL or PostgreSQL compatible databases. First, setup a KinD cluster: $ make -C deploy/kind kind ETCD Deploy a multi-tenant ETCD cluster into the Kamaji node: $ make -C deploy/kind etcd-cluster Now you're ready to install Kamaji operator . MySQL Deploy a MySQL/MariaDB backend into the Kamaji node: $ make -C deploy/kine/mysql mariadb Adjust the Kamaji install manifest according to the example of a MySQL DataStore and make sure Kamaji uses the proper datastore name: --datastore={.metadata.name} Now you're ready to install Kamaji operator . PostgreSQL Deploy a PostgreSQL backend into the Kamaji node: $ make -C deploy/kine/postgresql postgresql Adjust the Kamaji install manifest according to the example of a PostgreSQL DataStore and make sure Kamaji uses the proper datastore name: --datastore={.metadata.name} Now you're ready to install Kamaji operator . Install Kamaji $ kubectl apply -f config/install.yaml If you experience some errors during the apply of the manifest as resource mapping not found ... ensure CRDs are installed first , just apply it again. Deploy Tenant Control Plane Now it is the moment of deploying your first tenant control plane. $ kubectl apply -f - <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: tenant1 spec: controlPlane: deployment: replicas: 2 additionalMetadata: annotations: environment.clastix.io: tenant1 tier.clastix.io: \"0\" labels: tenant.clastix.io: tenant1 kind.clastix.io: deployment service: additionalMetadata: annotations: environment.clastix.io: tenant1 tier.clastix.io: \"0\" labels: tenant.clastix.io: tenant1 kind.clastix.io: service serviceType: NodePort kubernetes: version: \"v1.23.4\" kubelet: cgroupfs: cgroupfs admissionControllers: - LimitRanger - ResourceQuota networkProfile: address: \"172.18.0.2\" port: 31443 certSANs: - \"test.clastixlabs.io\" serviceCidr: \"10.96.0.0/16\" podCidr: \"10.244.0.0/16\" dnsServiceIPs: - \"10.96.0.10\" addons: coreDNS: {} kubeProxy: {} EOF Check networkProfile fields according to your installation To let Kamaji works in kind, you have indicate that the service must be NodePort Get Kubeconfig Let's retrieve kubeconfig and store in /tmp/kubeconfig $ kubectl get secrets tenant1-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 -d > /tmp/kubeconfig ``` It can be export it, to facilitate the next tasks: ```bash $ export KUBECONFIG=/tmp/kubeconfig Install CNI We highly recommend to install kindnet as CNI for your kamaji TCP. $ kubectl create -f https://raw.githubusercontent.com/aojea/kindnet/master/install-kindnet.yaml Join worker nodes $ make -C deploy/kind kamaji-kind-worker-join To add more worker nodes, run again the command above. Check out the node: $ kubectl get nodes NAME STATUS ROLES AGE VERSION d2d4b468c9de Ready <none> 44s v1.23.4 For more complex scenarios (exposing port, different version and so on), run join-node.bash . Tenant control plane provision has been finished in a minimal Kamaji setup based on KinD. Therefore, you could develop, test and make your own experiments with Kamaji. Cleanup $ make -C deploy/kind destroy","title":"Getting started"},{"location":"getting-started/#getting-started","text":"This document explains how to deploy a minimal Kamaji setup on KinD for development scopes. Please refer to the Kamaji documentation for understanding all the terms used in this guide, as for example: admin cluster , tenant cluster , and tenant control plane .","title":"Getting started"},{"location":"getting-started/#pre-requisites","text":"We assume you have installed on your workstation: Docker KinD kubectl@v1.25.0 kubeadm@v1.25.0 jq openssl cfssl cfssljson Starting from Kamaji v0.0.2, kubectl and kubeadm need to meet at least minimum version to v1.25.0 : this is required due to the latest changes addressed from the release Kubernetes 1.25 release regarding the kubelet-config ConfigMap required for the node join.","title":"Pre-requisites"},{"location":"getting-started/#setup-kamaji-on-kind","text":"The instance of Kamaji is made of a single node hosting: admin control-plane admin worker multi-tenant datastore","title":"Setup Kamaji on KinD"},{"location":"getting-started/#standard-installation","text":"You can install your KinD cluster, ETCD multi-tenant cluster and Kamaji operator with a single command : $ make -C deploy/kind Now you can create your first TenantControlPlane .","title":"Standard installation"},{"location":"getting-started/#data-store-specific","text":"Kamaji offers the possibility of using a different storage system than ETCD for the tenants, like MySQL or PostgreSQL compatible databases. First, setup a KinD cluster: $ make -C deploy/kind kind","title":"Data store-specific"},{"location":"getting-started/#etcd","text":"Deploy a multi-tenant ETCD cluster into the Kamaji node: $ make -C deploy/kind etcd-cluster Now you're ready to install Kamaji operator .","title":"ETCD"},{"location":"getting-started/#mysql","text":"Deploy a MySQL/MariaDB backend into the Kamaji node: $ make -C deploy/kine/mysql mariadb Adjust the Kamaji install manifest according to the example of a MySQL DataStore and make sure Kamaji uses the proper datastore name: --datastore={.metadata.name} Now you're ready to install Kamaji operator .","title":"MySQL"},{"location":"getting-started/#postgresql","text":"Deploy a PostgreSQL backend into the Kamaji node: $ make -C deploy/kine/postgresql postgresql Adjust the Kamaji install manifest according to the example of a PostgreSQL DataStore and make sure Kamaji uses the proper datastore name: --datastore={.metadata.name} Now you're ready to install Kamaji operator .","title":"PostgreSQL"},{"location":"getting-started/#install-kamaji","text":"$ kubectl apply -f config/install.yaml If you experience some errors during the apply of the manifest as resource mapping not found ... ensure CRDs are installed first , just apply it again.","title":"Install Kamaji"},{"location":"getting-started/#deploy-tenant-control-plane","text":"Now it is the moment of deploying your first tenant control plane. $ kubectl apply -f - <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: tenant1 spec: controlPlane: deployment: replicas: 2 additionalMetadata: annotations: environment.clastix.io: tenant1 tier.clastix.io: \"0\" labels: tenant.clastix.io: tenant1 kind.clastix.io: deployment service: additionalMetadata: annotations: environment.clastix.io: tenant1 tier.clastix.io: \"0\" labels: tenant.clastix.io: tenant1 kind.clastix.io: service serviceType: NodePort kubernetes: version: \"v1.23.4\" kubelet: cgroupfs: cgroupfs admissionControllers: - LimitRanger - ResourceQuota networkProfile: address: \"172.18.0.2\" port: 31443 certSANs: - \"test.clastixlabs.io\" serviceCidr: \"10.96.0.0/16\" podCidr: \"10.244.0.0/16\" dnsServiceIPs: - \"10.96.0.10\" addons: coreDNS: {} kubeProxy: {} EOF Check networkProfile fields according to your installation To let Kamaji works in kind, you have indicate that the service must be NodePort","title":"Deploy Tenant Control Plane"},{"location":"getting-started/#get-kubeconfig","text":"Let's retrieve kubeconfig and store in /tmp/kubeconfig $ kubectl get secrets tenant1-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 -d > /tmp/kubeconfig ``` It can be export it, to facilitate the next tasks: ```bash $ export KUBECONFIG=/tmp/kubeconfig","title":"Get Kubeconfig"},{"location":"getting-started/#install-cni","text":"We highly recommend to install kindnet as CNI for your kamaji TCP. $ kubectl create -f https://raw.githubusercontent.com/aojea/kindnet/master/install-kindnet.yaml","title":"Install CNI"},{"location":"getting-started/#join-worker-nodes","text":"$ make -C deploy/kind kamaji-kind-worker-join To add more worker nodes, run again the command above. Check out the node: $ kubectl get nodes NAME STATUS ROLES AGE VERSION d2d4b468c9de Ready <none> 44s v1.23.4 For more complex scenarios (exposing port, different version and so on), run join-node.bash . Tenant control plane provision has been finished in a minimal Kamaji setup based on KinD. Therefore, you could develop, test and make your own experiments with Kamaji.","title":"Join worker nodes"},{"location":"getting-started/#cleanup","text":"$ make -C deploy/kind destroy","title":"Cleanup"},{"location":"home/","text":"Kamaji Kamaji deploys and operates Kubernetes at scale with a fraction of the operational burden. How it works Kamaji turns any Kubernetes cluster into an \u201cadmin cluster\u201d to orchestrate other Kubernetes clusters called \u201ctenant clusters\u201d . What makes Kamaji special is that Control Planes of \u201ctenant clusters\u201d are just regular pods running in the \u201cadmin cluster\u201d instead of dedicated Virtual Machines. This solution makes running control planes at scale cheaper and easier to deploy and operate. View Concepts for a deeper understanding of principles behind Kamaji's design. All the tenant clusters built with Kamaji are fully compliant CNCF Certified Kubernetes and are compatible with the standard toolchains everybody knows and loves. Features Self Service Kubernetes: leave users the freedom to self-provision their Kubernetes clusters according to the assigned boundaries. Multi-cluster Management: centrally manage multiple tenant clusters from a single admin cluster. Happy SREs. Cheaper Control Planes: place multiple tenant control planes on a single node, instead of having three nodes for a single control plane. Stronger Multi-Tenancy: leave tenants to access the control plane with admin permissions while keeping the tenant isolated at the infrastructure level. Kubernetes Inception: use Kubernetes to manage Kubernetes by re-using all the Kubernetes goodies you already know and love. Full APIs compliant: tenant clusters are fully CNCF compliant built with upstream Kubernetes binaries. A user does not see differences between a Kamaji provisioned cluster and a dedicated cluster. Getting started Please refer to the Getting Started guide to deploy a minimal setup of Kamaji on KinD . Open Source Kamaji is Open Source with Apache 2 license and any contribution is welcome. Open an issue or suggest an enhancement on the GitHub project's page . Join the Kubernetes Slack Workspace and the #kamaji channel to meet end-users and contributors. FAQs Q. What does Kamaji means? A. Kamaji is named as the character Kamaji from the Japanese movie Spirited Away . Q. Is Kamaji another Kubernetes distribution? A. No, Kamaji is a Kubernetes Operator you can install on top of any Kubernetes cluster to provide hundreds of managed Kubernetes clusters as a service. We tested Kamaji on vanilla Kubernetes 1.22+, KinD, and Azure AKS. We expect it to work smoothly on other Kubernetes distributions. The tenant clusters made with Kamaji are conformant CNCF Kubernetes clusters as we leverage on kubeadm . Q. Is it safe to run Kubernetes control plane components in a pod instead of dedicated virtual machines? A. Yes, the tenant control plane components are packaged in the same way they are running in bare metal or virtual nodes. We leverage the kubeadm code to set up the control plane components as they were running on their own server. The unchanged images of upstream kube-apiserver , kube-scheduler , and kube-controller-manager are used. Q. You already provide a Kubernetes multi-tenancy solution with Capsule . Why does Kamaji matter? A. A multi-tenancy solution, like Capsule shares the Kubernetes control plane among all tenants keeping tenant namespaces isolated by policies. While the solution is the right choice by balancing between features and ease of usage, there are cases where a tenant user requires access to the control plane, for example, when a tenant requires to manage CRDs on his own. With Kamaji, you can provide cluster admin permissions to the tenant. Q. Well you convinced me, how to get a try? A. It is possible to get started with Kamaji on a laptop with KinD installed.","title":"Home"},{"location":"home/#kamaji","text":"Kamaji deploys and operates Kubernetes at scale with a fraction of the operational burden.","title":"Kamaji"},{"location":"home/#how-it-works","text":"Kamaji turns any Kubernetes cluster into an \u201cadmin cluster\u201d to orchestrate other Kubernetes clusters called \u201ctenant clusters\u201d . What makes Kamaji special is that Control Planes of \u201ctenant clusters\u201d are just regular pods running in the \u201cadmin cluster\u201d instead of dedicated Virtual Machines. This solution makes running control planes at scale cheaper and easier to deploy and operate. View Concepts for a deeper understanding of principles behind Kamaji's design. All the tenant clusters built with Kamaji are fully compliant CNCF Certified Kubernetes and are compatible with the standard toolchains everybody knows and loves.","title":"How it works"},{"location":"home/#features","text":"Self Service Kubernetes: leave users the freedom to self-provision their Kubernetes clusters according to the assigned boundaries. Multi-cluster Management: centrally manage multiple tenant clusters from a single admin cluster. Happy SREs. Cheaper Control Planes: place multiple tenant control planes on a single node, instead of having three nodes for a single control plane. Stronger Multi-Tenancy: leave tenants to access the control plane with admin permissions while keeping the tenant isolated at the infrastructure level. Kubernetes Inception: use Kubernetes to manage Kubernetes by re-using all the Kubernetes goodies you already know and love. Full APIs compliant: tenant clusters are fully CNCF compliant built with upstream Kubernetes binaries. A user does not see differences between a Kamaji provisioned cluster and a dedicated cluster.","title":"Features"},{"location":"home/#getting-started","text":"Please refer to the Getting Started guide to deploy a minimal setup of Kamaji on KinD .","title":"Getting started"},{"location":"home/#open-source","text":"Kamaji is Open Source with Apache 2 license and any contribution is welcome. Open an issue or suggest an enhancement on the GitHub project's page . Join the Kubernetes Slack Workspace and the #kamaji channel to meet end-users and contributors.","title":"Open Source"},{"location":"home/#faqs","text":"Q. What does Kamaji means? A. Kamaji is named as the character Kamaji from the Japanese movie Spirited Away . Q. Is Kamaji another Kubernetes distribution? A. No, Kamaji is a Kubernetes Operator you can install on top of any Kubernetes cluster to provide hundreds of managed Kubernetes clusters as a service. We tested Kamaji on vanilla Kubernetes 1.22+, KinD, and Azure AKS. We expect it to work smoothly on other Kubernetes distributions. The tenant clusters made with Kamaji are conformant CNCF Kubernetes clusters as we leverage on kubeadm . Q. Is it safe to run Kubernetes control plane components in a pod instead of dedicated virtual machines? A. Yes, the tenant control plane components are packaged in the same way they are running in bare metal or virtual nodes. We leverage the kubeadm code to set up the control plane components as they were running on their own server. The unchanged images of upstream kube-apiserver , kube-scheduler , and kube-controller-manager are used. Q. You already provide a Kubernetes multi-tenancy solution with Capsule . Why does Kamaji matter? A. A multi-tenancy solution, like Capsule shares the Kubernetes control plane among all tenants keeping tenant namespaces isolated by policies. While the solution is the right choice by balancing between features and ease of usage, there are cases where a tenant user requires access to the control plane, for example, when a tenant requires to manage CRDs on his own. With Kamaji, you can provide cluster admin permissions to the tenant. Q. Well you convinced me, how to get a try? A. It is possible to get started with Kamaji on a laptop with KinD installed.","title":"FAQs"},{"location":"use-cases/","text":"Use Cases Kamaji project has been initially started as a solution for actual and common problems such as minimizing the Total Cost of Ownership while running Kubernetes at large scale. However, it can open a wider range of use cases. Here are a few: Managed Kubernetes: enable companies to provide Cloud Native Infrastructure with ease by introducing a strong separation of concerns between management and workloads. Centralize clusters management, monitoring, and observability by leaving developers to focus on applications, increase productivity and reduce operational costs. Kubernetes as a Service: provide Kubernetes clusters in a self-service fashion by running management and workloads on different infrastructures with the option of Bring Your Own Device, BYOD. Control Plane as a Service: provide multiple Kubernetes control planes running on top of a single Kubernetes cluster. Tenants who use namespaces based isolation often still need access to cluster wide resources like Cluster Roles, Admission Webhooks, or Custom Resource Definitions. Edge Computing: distribute Kubernetes workloads across edge computing locations without having to manage multiple clusters across various providers. Centralize management of hundreds of control planes while leaving workloads to run isolated on their own dedicated infrastructure. Cluster Simulation: check new Kubernetes API or experimental flag or a new tool without impacting production operations. Kamaji will let you simulate such things in a safe and controlled environment. Workloads Testing: check the behaviour of your workloads on different and multiple versions of Kubernetes with ease by deploying multiple Control Planes in a single cluster.","title":"Use Cases"},{"location":"use-cases/#use-cases","text":"Kamaji project has been initially started as a solution for actual and common problems such as minimizing the Total Cost of Ownership while running Kubernetes at large scale. However, it can open a wider range of use cases. Here are a few: Managed Kubernetes: enable companies to provide Cloud Native Infrastructure with ease by introducing a strong separation of concerns between management and workloads. Centralize clusters management, monitoring, and observability by leaving developers to focus on applications, increase productivity and reduce operational costs. Kubernetes as a Service: provide Kubernetes clusters in a self-service fashion by running management and workloads on different infrastructures with the option of Bring Your Own Device, BYOD. Control Plane as a Service: provide multiple Kubernetes control planes running on top of a single Kubernetes cluster. Tenants who use namespaces based isolation often still need access to cluster wide resources like Cluster Roles, Admission Webhooks, or Custom Resource Definitions. Edge Computing: distribute Kubernetes workloads across edge computing locations without having to manage multiple clusters across various providers. Centralize management of hundreds of control planes while leaving workloads to run isolated on their own dedicated infrastructure. Cluster Simulation: check new Kubernetes API or experimental flag or a new tool without impacting production operations. Kamaji will let you simulate such things in a safe and controlled environment. Workloads Testing: check the behaviour of your workloads on different and multiple versions of Kubernetes with ease by deploying multiple Control Planes in a single cluster.","title":"Use Cases"},{"location":"guides/","text":"How to Guides This section of the Kamaji documentation contains pages that show how to do a specific thing, typically by giving a sequence of steps.","title":"How to Guides"},{"location":"guides/#how-to-guides","text":"This section of the Kamaji documentation contains pages that show how to do a specific thing, typically by giving a sequence of steps.","title":"How to Guides"},{"location":"guides/kamaji-azure-deployment-guide/","text":"Setup Kamaji on Azure This guide will lead you through the process of creating a working Kamaji setup on on MS Azure. The material here is relatively dense. We strongly encourage you to dedicate time to walk through these instructions, with a mind to learning. We do NOT provide any \"one-click\" deployment here. However, once you've understood the components involved it is encouraged that you build suitable, auditable GitOps deployment processes around your final infrastructure. The guide requires: one bootstrap workstation an AKS Kubernetes cluster to run the Admin and Tenant Control Planes an arbitrary number of Azure virtual machines to host Tenant s' workloads Summary Prepare the bootstrap workspace Access Admin cluster Install DataStore Install Kamaji controller Create Tenant Cluster Cleanup Prepare the bootstrap workspace This guide is supposed to be run from a remote or local bootstrap machine. First, clone the repo and prepare the workspace directory: git clone https://github.com/clastix/kamaji cd kamaji/deploy We assume you have installed on your workstation: kubectl kubeadm helm jq Azure CLI Make sure you have a valid Azure subscription, and login to Azure: az account set --subscription \"MySubscription\" az login Currently, the Kamaji setup, including Admin and Tenant clusters need to be deployed within the same Azure region. Cross-regions deployments are not supported. Access Admin cluster In Kamaji, an Admin Cluster is a regular Kubernetes cluster which hosts zero to many Tenant Cluster Control Planes. The admin cluster acts as management cluster for all the Tenant clusters and implements Monitoring, Logging, and Governance of all the Kamaji setup, including all Tenant clusters. For this guide, we're going to use an instance of Azure Kubernetes Service - AKS as the Admin Cluster. Throughout the following instructions, shell variables are used to indicate values that you should adjust to your own Azure environment: source kamaji-azure.env az group create \\ --name $KAMAJI_RG \\ --location $KAMAJI_REGION az network vnet create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_VNET_NAME \\ --location $KAMAJI_REGION \\ --address-prefix $KAMAJI_VNET_ADDRESS az network vnet subnet create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_SUBNET_NAME \\ --vnet-name $KAMAJI_VNET_NAME \\ --address-prefixes $KAMAJI_SUBNET_ADDRESS KAMAJI_SUBNET_ID=$(az network vnet subnet show \\ --resource-group ${KAMAJI_RG} \\ --vnet-name ${KAMAJI_VNET_NAME} \\ --name ${KAMAJI_SUBNET_NAME} \\ --query id --output tsv) az aks create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_CLUSTER \\ --location $KAMAJI_REGION \\ --vnet-subnet-id $KAMAJI_SUBNET_ID \\ --zones 1 2 3 \\ --node-count 3 \\ --nodepool-name $KAMAJI_CLUSTER Once the cluster formation succedes, get credentials to access the cluster as admin az aks get-credentials \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_CLUSTER And check you can access: kubectl cluster-info Install datastore The Kamaji controller needs to access a multi-tenant datastore in order to save data of the tenants' clusters. The Helm Chart provides the installation of an unamanaged etcd . However, a managed etcd is highly recommended in production. The kamaji-etcd project provides a viable option to setup a manged multi-tenant etcd as 3 replicas StatefulSet with data persistence: helm repo add clastix https://clastix.github.io/charts helm repo update helm install etcd clastix/kamaji-etcd -n kamaji-system --create-namespace Optionally, Kamaji offers the possibility of using a different storage system for the tenants' clusters, as MySQL or PostgreSQL compatible database, thanks to the native kine integration. Install Kamaji Controller There are multiple ways to deploy Kamaji, including a single YAML file and the Helm Chart . Install with helm using an unmanaged etcd as datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace Alternatively, if you opted for a managed etcd datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace --set etcd.deploy=false Congratulations! You just turned your Azure Kubernetes AKS cluster into a Kamaji cluster capable to run multiple Tenant Control Planes. Create Tenant Cluster Tenant Control Plane With Kamaji on AKS, the tenant control plane is accessible: from tenant worker nodes through an internal loadbalancer from tenant admin user through an external loadbalancer responding to https://${TENANT_NAME}.${TENANT_NAME}.${TENANT_DOMAIN}:443 Create a tenant control plane of example: cat > ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: ${TENANT_NAME} namespace: ${TENANT_NAMESPACE} spec: controlPlane: deployment: replicas: 3 additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} extraArgs: apiServer: [] controllerManager: [] scheduler: [] resources: apiServer: requests: cpu: 250m memory: 512Mi limits: {} controllerManager: requests: cpu: 125m memory: 256Mi limits: {} scheduler: requests: cpu: 125m memory: 256Mi limits: {} service: additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" serviceType: LoadBalancer kubernetes: version: ${TENANT_VERSION} kubelet: cgroupfs: systemd admissionControllers: - ResourceQuota - LimitRanger networkProfile: port: ${TENANT_PORT} certSANs: - ${TENANT_NAME}.${TENANT_DOMAIN} serviceCidr: ${TENANT_SVC_CIDR} podCidr: ${TENANT_POD_CIDR} dnsServiceIPs: - ${TENANT_DNS_SERVICE} addons: coreDNS: {} kubeProxy: {} konnectivity: proxyPort: ${TENANT_PROXY_PORT} resources: requests: cpu: 100m memory: 128Mi limits: {} --- apiVersion: v1 kind: Service metadata: name: ${TENANT_NAME}-public namespace: ${TENANT_NAMESPACE} annotations: service.beta.kubernetes.io/azure-dns-label-name: ${TENANT_NAME} spec: ports: - port: 443 protocol: TCP targetPort: ${TENANT_PORT} selector: kamaji.clastix.io/soot: ${TENANT_NAME} type: LoadBalancer EOF kubectl -n ${TENANT_NAMESPACE} apply -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml Make sure: the following annotation: service.beta.kubernetes.io/azure-load-balancer-internal=true is set on the tcp service. It tells Azure to expose the service within an internal loadbalancer. the following annotation: service.beta.kubernetes.io/azure-dns-label-name=${TENANT_NAME} is set the public loadbalancer service. It tells Azure to expose the Tenant Control Plane with public domain name: ${TENANT_NAME}.${TENANT_DOMAIN} . Working with Tenant Control Plane Check the access to the Tenant Control Plane: curl -k https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com/healthz curl -k https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com/version Let's retrieve the kubeconfig in order to work with it: kubectl get secrets -n ${TENANT_NAMESPACE} ${TENANT_NAME}-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 --decode \\ > ${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig config \\ set-cluster ${TENANT_NAME} \\ --server https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com and let's check it out: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get svc NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 6m Check out how the Tenant Control Plane advertises itself: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get ep NAME ENDPOINTS AGE kubernetes 10.240.0.100:6443 57m Preparing Worker Nodes to join Currently Kamaji does not provide any helper for creation of tenant worker nodes. You should get a set of machines from your infrastructure provider, turn them into worker nodes, and then join to the tenant control plane with the kubeadm . In the future, we'll provide integration with Cluster APIs and other tools, as for example, Terrform. Create an Azure VM Stateful Set to host worker nodes az network vnet subnet create \\ --resource-group $KAMAJI_RG \\ --name $TENANT_SUBNET_NAME \\ --vnet-name $KAMAJI_VNET_NAME \\ --address-prefixes $TENANT_SUBNET_ADDRESS az vmss create \\ --name $TENANT_VMSS \\ --resource-group $KAMAJI_RG \\ --image $TENANT_VM_IMAGE \\ --vnet-name $KAMAJI_VNET_NAME \\ --subnet $TENANT_SUBNET_NAME \\ --computer-name-prefix $TENANT_NAME- \\ --custom-data ./tenant-cloudinit.yaml \\ --load-balancer \"\" \\ --instance-count 0 az vmss update \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --set virtualMachineProfile.networkProfile.networkInterfaceConfigurations[0].enableIPForwarding=true az vmss scale \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --new-capacity 3 Join the tenant virtual machines to the tenant control plane The current approach for joining nodes is to use kubeadm and therefore, we will create a bootstrap token to perform the action. In order to facilitate the step, we will store the entire command of joining in a variable: TENANT_ADDR=$(kubectl -n ${TENANT_NAMESPACE} get svc ${TENANT_NAME} -o json | jq -r .\"spec.loadBalancerIP\") JOIN_CMD=$(echo \"sudo kubeadm join ${TENANT_ADDR}:6443 \")$(kubeadm --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig token create --print-join-command |cut -d\" \" -f4-) A bash loop will be used to join all the available nodes. VMIDS=($(az vmss list-instances \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --query [].instanceId \\ --output tsv)) for i in ${!VMIDS[@]}; do VMID=${VMIDS[$i]} az vmss run-command create \\ --name join-tenant-control-plane \\ --vmss-name $TENANT_VMSS \\ --resource-group $KAMAJI_RG \\ --instance-id ${VMID} \\ --script \"${JOIN_CMD}\" done Checking the nodes: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-000000 NotReady <none> 112s v1.25.0 tenant-00-000002 NotReady <none> 92s v1.25.0 tenant-00-000003 NotReady <none> 71s v1.25.0 The cluster needs a CNI plugin to get the nodes ready. In this guide, we are going to install calico , but feel free to use one of your taste. Download the latest stable Calico manifest: curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O As per documentation , Calico in VXLAN mode is supported on Azure while IPIP packets are blocked by the Azure network fabric. Make sure you edit the manifest above and set the following variables: CLUSTER_TYPE=\"k8s\" CALICO_IPV4POOL_IPIP=\"Never\" CALICO_IPV4POOL_VXLAN=\"Always\" Apply to the tenant cluster: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig apply -f calico.yaml And after a while, nodes will be ready kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-000000 Ready <none> 3m38s v1.25.0 tenant-00-000002 Ready <none> 3m18s v1.25.0 tenant-00-000003 Ready <none> 2m57s v1.25.0 Cleanup To get rid of the Kamaji infrastructure, remove the RESOURCE_GROUP: az group delete --name $KAMAJI_RG --yes --no-wait That's all folks!","title":"Setup Kamaji on Azure"},{"location":"guides/kamaji-azure-deployment-guide/#setup-kamaji-on-azure","text":"This guide will lead you through the process of creating a working Kamaji setup on on MS Azure. The material here is relatively dense. We strongly encourage you to dedicate time to walk through these instructions, with a mind to learning. We do NOT provide any \"one-click\" deployment here. However, once you've understood the components involved it is encouraged that you build suitable, auditable GitOps deployment processes around your final infrastructure. The guide requires: one bootstrap workstation an AKS Kubernetes cluster to run the Admin and Tenant Control Planes an arbitrary number of Azure virtual machines to host Tenant s' workloads","title":"Setup Kamaji on Azure"},{"location":"guides/kamaji-azure-deployment-guide/#summary","text":"Prepare the bootstrap workspace Access Admin cluster Install DataStore Install Kamaji controller Create Tenant Cluster Cleanup","title":"Summary"},{"location":"guides/kamaji-azure-deployment-guide/#prepare-the-bootstrap-workspace","text":"This guide is supposed to be run from a remote or local bootstrap machine. First, clone the repo and prepare the workspace directory: git clone https://github.com/clastix/kamaji cd kamaji/deploy We assume you have installed on your workstation: kubectl kubeadm helm jq Azure CLI Make sure you have a valid Azure subscription, and login to Azure: az account set --subscription \"MySubscription\" az login Currently, the Kamaji setup, including Admin and Tenant clusters need to be deployed within the same Azure region. Cross-regions deployments are not supported.","title":"Prepare the bootstrap workspace"},{"location":"guides/kamaji-azure-deployment-guide/#access-admin-cluster","text":"In Kamaji, an Admin Cluster is a regular Kubernetes cluster which hosts zero to many Tenant Cluster Control Planes. The admin cluster acts as management cluster for all the Tenant clusters and implements Monitoring, Logging, and Governance of all the Kamaji setup, including all Tenant clusters. For this guide, we're going to use an instance of Azure Kubernetes Service - AKS as the Admin Cluster. Throughout the following instructions, shell variables are used to indicate values that you should adjust to your own Azure environment: source kamaji-azure.env az group create \\ --name $KAMAJI_RG \\ --location $KAMAJI_REGION az network vnet create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_VNET_NAME \\ --location $KAMAJI_REGION \\ --address-prefix $KAMAJI_VNET_ADDRESS az network vnet subnet create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_SUBNET_NAME \\ --vnet-name $KAMAJI_VNET_NAME \\ --address-prefixes $KAMAJI_SUBNET_ADDRESS KAMAJI_SUBNET_ID=$(az network vnet subnet show \\ --resource-group ${KAMAJI_RG} \\ --vnet-name ${KAMAJI_VNET_NAME} \\ --name ${KAMAJI_SUBNET_NAME} \\ --query id --output tsv) az aks create \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_CLUSTER \\ --location $KAMAJI_REGION \\ --vnet-subnet-id $KAMAJI_SUBNET_ID \\ --zones 1 2 3 \\ --node-count 3 \\ --nodepool-name $KAMAJI_CLUSTER Once the cluster formation succedes, get credentials to access the cluster as admin az aks get-credentials \\ --resource-group $KAMAJI_RG \\ --name $KAMAJI_CLUSTER And check you can access: kubectl cluster-info","title":"Access Admin cluster"},{"location":"guides/kamaji-azure-deployment-guide/#install-datastore","text":"The Kamaji controller needs to access a multi-tenant datastore in order to save data of the tenants' clusters. The Helm Chart provides the installation of an unamanaged etcd . However, a managed etcd is highly recommended in production. The kamaji-etcd project provides a viable option to setup a manged multi-tenant etcd as 3 replicas StatefulSet with data persistence: helm repo add clastix https://clastix.github.io/charts helm repo update helm install etcd clastix/kamaji-etcd -n kamaji-system --create-namespace Optionally, Kamaji offers the possibility of using a different storage system for the tenants' clusters, as MySQL or PostgreSQL compatible database, thanks to the native kine integration.","title":"Install datastore"},{"location":"guides/kamaji-azure-deployment-guide/#install-kamaji-controller","text":"There are multiple ways to deploy Kamaji, including a single YAML file and the Helm Chart . Install with helm using an unmanaged etcd as datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace Alternatively, if you opted for a managed etcd datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace --set etcd.deploy=false Congratulations! You just turned your Azure Kubernetes AKS cluster into a Kamaji cluster capable to run multiple Tenant Control Planes.","title":"Install Kamaji Controller"},{"location":"guides/kamaji-azure-deployment-guide/#create-tenant-cluster","text":"","title":"Create Tenant Cluster"},{"location":"guides/kamaji-azure-deployment-guide/#tenant-control-plane","text":"With Kamaji on AKS, the tenant control plane is accessible: from tenant worker nodes through an internal loadbalancer from tenant admin user through an external loadbalancer responding to https://${TENANT_NAME}.${TENANT_NAME}.${TENANT_DOMAIN}:443 Create a tenant control plane of example: cat > ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: ${TENANT_NAME} namespace: ${TENANT_NAMESPACE} spec: controlPlane: deployment: replicas: 3 additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} extraArgs: apiServer: [] controllerManager: [] scheduler: [] resources: apiServer: requests: cpu: 250m memory: 512Mi limits: {} controllerManager: requests: cpu: 125m memory: 256Mi limits: {} scheduler: requests: cpu: 125m memory: 256Mi limits: {} service: additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" serviceType: LoadBalancer kubernetes: version: ${TENANT_VERSION} kubelet: cgroupfs: systemd admissionControllers: - ResourceQuota - LimitRanger networkProfile: port: ${TENANT_PORT} certSANs: - ${TENANT_NAME}.${TENANT_DOMAIN} serviceCidr: ${TENANT_SVC_CIDR} podCidr: ${TENANT_POD_CIDR} dnsServiceIPs: - ${TENANT_DNS_SERVICE} addons: coreDNS: {} kubeProxy: {} konnectivity: proxyPort: ${TENANT_PROXY_PORT} resources: requests: cpu: 100m memory: 128Mi limits: {} --- apiVersion: v1 kind: Service metadata: name: ${TENANT_NAME}-public namespace: ${TENANT_NAMESPACE} annotations: service.beta.kubernetes.io/azure-dns-label-name: ${TENANT_NAME} spec: ports: - port: 443 protocol: TCP targetPort: ${TENANT_PORT} selector: kamaji.clastix.io/soot: ${TENANT_NAME} type: LoadBalancer EOF kubectl -n ${TENANT_NAMESPACE} apply -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml Make sure: the following annotation: service.beta.kubernetes.io/azure-load-balancer-internal=true is set on the tcp service. It tells Azure to expose the service within an internal loadbalancer. the following annotation: service.beta.kubernetes.io/azure-dns-label-name=${TENANT_NAME} is set the public loadbalancer service. It tells Azure to expose the Tenant Control Plane with public domain name: ${TENANT_NAME}.${TENANT_DOMAIN} .","title":"Tenant Control Plane"},{"location":"guides/kamaji-azure-deployment-guide/#working-with-tenant-control-plane","text":"Check the access to the Tenant Control Plane: curl -k https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com/healthz curl -k https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com/version Let's retrieve the kubeconfig in order to work with it: kubectl get secrets -n ${TENANT_NAMESPACE} ${TENANT_NAME}-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 --decode \\ > ${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig config \\ set-cluster ${TENANT_NAME} \\ --server https://${TENANT_NAME}.${KAMAJI_REGION}.cloudapp.azure.com and let's check it out: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get svc NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 6m Check out how the Tenant Control Plane advertises itself: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get ep NAME ENDPOINTS AGE kubernetes 10.240.0.100:6443 57m","title":"Working with Tenant Control Plane"},{"location":"guides/kamaji-azure-deployment-guide/#preparing-worker-nodes-to-join","text":"Currently Kamaji does not provide any helper for creation of tenant worker nodes. You should get a set of machines from your infrastructure provider, turn them into worker nodes, and then join to the tenant control plane with the kubeadm . In the future, we'll provide integration with Cluster APIs and other tools, as for example, Terrform. Create an Azure VM Stateful Set to host worker nodes az network vnet subnet create \\ --resource-group $KAMAJI_RG \\ --name $TENANT_SUBNET_NAME \\ --vnet-name $KAMAJI_VNET_NAME \\ --address-prefixes $TENANT_SUBNET_ADDRESS az vmss create \\ --name $TENANT_VMSS \\ --resource-group $KAMAJI_RG \\ --image $TENANT_VM_IMAGE \\ --vnet-name $KAMAJI_VNET_NAME \\ --subnet $TENANT_SUBNET_NAME \\ --computer-name-prefix $TENANT_NAME- \\ --custom-data ./tenant-cloudinit.yaml \\ --load-balancer \"\" \\ --instance-count 0 az vmss update \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --set virtualMachineProfile.networkProfile.networkInterfaceConfigurations[0].enableIPForwarding=true az vmss scale \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --new-capacity 3","title":"Preparing Worker Nodes to join"},{"location":"guides/kamaji-azure-deployment-guide/#join-the-tenant-virtual-machines-to-the-tenant-control-plane","text":"The current approach for joining nodes is to use kubeadm and therefore, we will create a bootstrap token to perform the action. In order to facilitate the step, we will store the entire command of joining in a variable: TENANT_ADDR=$(kubectl -n ${TENANT_NAMESPACE} get svc ${TENANT_NAME} -o json | jq -r .\"spec.loadBalancerIP\") JOIN_CMD=$(echo \"sudo kubeadm join ${TENANT_ADDR}:6443 \")$(kubeadm --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig token create --print-join-command |cut -d\" \" -f4-) A bash loop will be used to join all the available nodes. VMIDS=($(az vmss list-instances \\ --resource-group $KAMAJI_RG \\ --name $TENANT_VMSS \\ --query [].instanceId \\ --output tsv)) for i in ${!VMIDS[@]}; do VMID=${VMIDS[$i]} az vmss run-command create \\ --name join-tenant-control-plane \\ --vmss-name $TENANT_VMSS \\ --resource-group $KAMAJI_RG \\ --instance-id ${VMID} \\ --script \"${JOIN_CMD}\" done Checking the nodes: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-000000 NotReady <none> 112s v1.25.0 tenant-00-000002 NotReady <none> 92s v1.25.0 tenant-00-000003 NotReady <none> 71s v1.25.0 The cluster needs a CNI plugin to get the nodes ready. In this guide, we are going to install calico , but feel free to use one of your taste. Download the latest stable Calico manifest: curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O As per documentation , Calico in VXLAN mode is supported on Azure while IPIP packets are blocked by the Azure network fabric. Make sure you edit the manifest above and set the following variables: CLUSTER_TYPE=\"k8s\" CALICO_IPV4POOL_IPIP=\"Never\" CALICO_IPV4POOL_VXLAN=\"Always\" Apply to the tenant cluster: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig apply -f calico.yaml And after a while, nodes will be ready kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-000000 Ready <none> 3m38s v1.25.0 tenant-00-000002 Ready <none> 3m18s v1.25.0 tenant-00-000003 Ready <none> 2m57s v1.25.0","title":"Join the tenant virtual machines to the tenant control plane"},{"location":"guides/kamaji-azure-deployment-guide/#cleanup","text":"To get rid of the Kamaji infrastructure, remove the RESOURCE_GROUP: az group delete --name $KAMAJI_RG --yes --no-wait That's all folks!","title":"Cleanup"},{"location":"guides/kamaji-deployment-guide/","text":"Setup Kamaji on a generic infrastructure This guide will lead you through the process of creating a working Kamaji setup on a generic infrastructure, both virtual or bare metal. The material here is relatively dense. We strongly encourage you to dedicate time to walk through these instructions, with a mind to learning. We do NOT provide any \"one-click\" deployment here. However, once you've understood the components involved it is encouraged that you build suitable, auditable GitOps deployment processes around your final infrastructure. The guide requires: one bootstrap workstation a Kubernetes cluster to run the Admin and Tenant Control Planes an arbitrary number of machines to host Tenant s' workloads Summary Prepare the bootstrap workspace Access Admin cluster Install DataStore Install Kamaji controller Create Tenant Cluster Cleanup Prepare the bootstrap workspace This guide is supposed to be run from a remote or local bootstrap machine. First, clone the repo and prepare the workspace directory: git clone https://github.com/clastix/kamaji cd kamaji/deploy We assume you have installed on your workstation: kubectl kubeadm helm jq Access Admin cluster In Kamaji, an Admin Cluster is a regular Kubernetes cluster which hosts zero to many Tenant Cluster Control Planes. The admin cluster acts as management cluster for all the Tenant clusters and implements Monitoring, Logging, and Governance of all the Kamaji setup, including all Tenant clusters. Throughout the following instructions, shell variables are used to indicate values that you should adjust to your environment, as of kamaji.env : source kamaji.env Any regular and conformant Kubernetes v1.22+ cluster can be turned into a Kamaji setup. To work properly, the admin cluster should provide at least: CNI module installed, eg. Calico , Cilium . CSI module installed with a Storage Class for the Tenants' etcd . Local Persistent Volumes are an option. Support for LoadBalancer Service Type, or alternatively, an Ingress Controller, eg. ingress-nginx , haproxy . Monitoring Stack, eg. Prometheus . Make sure you have a kubeconfig file with admin permissions on the cluster you want to turn into Kamaji Admin Cluster. Install datastore The Kamaji controller needs to access a multi-tenant datastore in order to save data of the tenants' clusters. The Helm Chart provides the installation of an unamanaged etcd . However, a managed etcd is highly recommended in production. The kamaji-etcd project provides a viable option to setup a manged multi-tenant etcd as 3 replicas StatefulSet with data persistence: helm repo add clastix https://clastix.github.io/charts helm repo update helm install etcd clastix/kamaji-etcd -n kamaji-system --create-namespace Optionally, Kamaji offers the possibility of using a different storage system for the tenants' clusters, as MySQL or PostgreSQL compatible database, thanks to the native kine integration. Install Kamaji Controller There are multiple ways to deploy Kamaji, including a single YAML file and the Helm Chart . Install with helm using an unmanaged etcd as datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace Alternatively, if you opted for a managed etcd datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace --set etcd.deploy=false Congratulations! You just turned your Kubernetes cluster into a Kamaji cluster capable to run multiple Tenant Control Planes. Create Tenant Cluster Tenant Control Plane A tenant control plane of example looks like: cat > ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: ${TENANT_NAME} namespace: ${TENANT_NAMESPACE} spec: controlPlane: deployment: replicas: 3 additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} extraArgs: apiServer: [] controllerManager: [] scheduler: [] resources: apiServer: requests: cpu: 250m memory: 512Mi limits: {} controllerManager: requests: cpu: 125m memory: 256Mi limits: {} scheduler: requests: cpu: 125m memory: 256Mi limits: {} service: additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} serviceType: LoadBalancer kubernetes: version: ${TENANT_VERSION} kubelet: cgroupfs: systemd admissionControllers: - ResourceQuota - LimitRanger networkProfile: port: ${TENANT_PORT} certSANs: - ${TENANT_NAME}.${TENANT_DOMAIN} serviceCidr: ${TENANT_SVC_CIDR} podCidr: ${TENANT_POD_CIDR} dnsServiceIPs: - ${TENANT_DNS_SERVICE} addons: coreDNS: {} kubeProxy: {} konnectivity: proxyPort: ${TENANT_PROXY_PORT} resources: requests: cpu: 100m memory: 128Mi limits: {} EOF kubectl -n ${TENANT_NAMESPACE} apply -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml After a few minutes, check the created resources in the tenants namespace and when ready it will look similar to the following: kubectl -n tenants get tcp,deploy,pods,svc NAME VERSION STATUS CONTROL-PLANE-ENDPOINT KUBECONFIG AGE tenantcontrolplane.kamaji.clastix.io/tenant-00 v1.23.1 Ready 192.168.32.240:6443 tenant-00-admin-kubeconfig 2m20s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/tenant-00 3/3 3 3 118s NAME READY STATUS RESTARTS AGE pod/tenant-00-58847c8cdd-7hc4n 4/4 Running 0 82s pod/tenant-00-58847c8cdd-ft5xt 4/4 Running 0 82s pod/tenant-00-58847c8cdd-shc7t 4/4 Running 0 82s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tenant-00 LoadBalancer 10.32.132.241 192.168.32.240 6443:32152/TCP,8132:32713/TCP 2m20s The regular Tenant Control Plane containers: kube-apiserver , kube-controller-manager , kube-scheduler are running unchanged in the tcp pods instead of dedicated machines and they are exposed through a service on the port 6443 of worker nodes in the Admin cluster. The LoadBalancer service type is used to expose the Tenant Control Plane. However, NodePort and ClusterIP with an Ingress Controller are still viable options, depending on the case. High Availability and rolling updates of the Tenant Control Plane are provided by the tcp Deployment and all the resources reconcilied by the Kamaji controller. Working with Tenant Control Plane Collect the external IP address of the tcp service: TENANT_ADDR=$(kubectl -n ${TENANT_NAMESPACE} get svc ${TENANT_NAME} -o json | jq -r .\"spec.loadBalancerIP\") and check it out: curl -k https://${TENANT_ADDR}:${TENANT_PORT}/healthz curl -k https://${TENANT_ADDR}:${TENANT_PORT}/version The kubeconfig required to access the Tenant Control Plane is stored in a secret: kubectl get secrets -n ${TENANT_NAMESPACE} ${TENANT_NAME}-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 --decode \\ > ${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig and let's check it out: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig cluster-info Kubernetes control plane is running at https://192.168.32.240:6443 CoreDNS is running at https://192.168.32.240:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Check out how the Tenant control Plane advertises itself to workloads: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get svc NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 6m kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get ep NAME ENDPOINTS AGE kubernetes 192.168.32.240:6443 18m And make sure it is ${TENANT_ADDR}:${TENANT_PORT} . Preparing Worker Nodes to join Currently Kamaji does not provide any helper for creation of tenant worker nodes. You should get a set of machines from your infrastructure provider, turn them into worker nodes, and then join to the tenant control plane with the kubeadm . In the future, we'll provide integration with Cluster APIs and other tools, as for example, Terraform. You can use the provided helper script nodes-prerequisites.sh , in order to install the dependencies on all the worker nodes: Install containerd as container runtime Install crictl , the command line for working with containerd Install kubectl , kubelet , and kubeadm in the desired version Warning: the script assumes all worker nodes are running Ubuntu 20.04 . Make sure to adapt the script if you're using a different distribution. Run the script: HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) ./nodes-prerequisites.sh ${TENANT_VERSION:1} ${HOSTS[@]} Join Command The current approach for joining nodes is to use kubeadm and therefore, we will create a bootstrap token to perform the action. In order to facilitate the step, we will store the entire command of joining in a variable: JOIN_CMD=$(echo \"sudo \")$(kubeadm --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig token create --print-join-command) Adding Worker Nodes A bash loop will be used to join all the available nodes. HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) for i in \"${!HOSTS[@]}\"; do HOST=${HOSTS[$i]} ssh ${USER}@${HOST} -t ${JOIN_CMD}; done Checking the nodes: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-worker-00 NotReady <none> 25s v1.25.0 tenant-00-worker-01 NotReady <none> 17s v1.25.0 tenant-00-worker-02 NotReady <none> 9s v1.25.0 The cluster needs a CNI plugin to get the nodes ready. In this guide, we are going to install calico , but feel free to use one of your taste. Download the latest stable Calico manifest: curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O Before to apply the Calico manifest, you can customize it as necessary according to your preferences. Apply to the tenant cluster: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig apply -f calico.yaml And after a while, nodes will be ready kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-worker-00 Ready <none> 2m48s v1.25.0 tenant-00-worker-01 Ready <none> 2m40s v1.25.0 tenant-00-worker-02 Ready <none> 2m32s v1.25.0 Cleanup Remove the worker nodes joined the tenant control plane kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig delete nodes --all For each worker node, login and clean it HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) for i in \"${!HOSTS[@]}\"; do HOST=${HOSTS[$i]} ssh ${USER}@${HOST} -t 'sudo kubeadm reset -f'; ssh ${USER}@${HOST} -t 'sudo rm -rf /etc/cni/net.d'; ssh ${USER}@${HOST} -t 'sudo systemctl reboot'; done Delete the tenant control plane from kamaji kubectl delete -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml That's all folks!","title":"Setup Kamaji on a generic infrastructure"},{"location":"guides/kamaji-deployment-guide/#setup-kamaji-on-a-generic-infrastructure","text":"This guide will lead you through the process of creating a working Kamaji setup on a generic infrastructure, both virtual or bare metal. The material here is relatively dense. We strongly encourage you to dedicate time to walk through these instructions, with a mind to learning. We do NOT provide any \"one-click\" deployment here. However, once you've understood the components involved it is encouraged that you build suitable, auditable GitOps deployment processes around your final infrastructure. The guide requires: one bootstrap workstation a Kubernetes cluster to run the Admin and Tenant Control Planes an arbitrary number of machines to host Tenant s' workloads","title":"Setup Kamaji on a generic infrastructure"},{"location":"guides/kamaji-deployment-guide/#summary","text":"Prepare the bootstrap workspace Access Admin cluster Install DataStore Install Kamaji controller Create Tenant Cluster Cleanup","title":"Summary"},{"location":"guides/kamaji-deployment-guide/#prepare-the-bootstrap-workspace","text":"This guide is supposed to be run from a remote or local bootstrap machine. First, clone the repo and prepare the workspace directory: git clone https://github.com/clastix/kamaji cd kamaji/deploy We assume you have installed on your workstation: kubectl kubeadm helm jq","title":"Prepare the bootstrap workspace"},{"location":"guides/kamaji-deployment-guide/#access-admin-cluster","text":"In Kamaji, an Admin Cluster is a regular Kubernetes cluster which hosts zero to many Tenant Cluster Control Planes. The admin cluster acts as management cluster for all the Tenant clusters and implements Monitoring, Logging, and Governance of all the Kamaji setup, including all Tenant clusters. Throughout the following instructions, shell variables are used to indicate values that you should adjust to your environment, as of kamaji.env : source kamaji.env Any regular and conformant Kubernetes v1.22+ cluster can be turned into a Kamaji setup. To work properly, the admin cluster should provide at least: CNI module installed, eg. Calico , Cilium . CSI module installed with a Storage Class for the Tenants' etcd . Local Persistent Volumes are an option. Support for LoadBalancer Service Type, or alternatively, an Ingress Controller, eg. ingress-nginx , haproxy . Monitoring Stack, eg. Prometheus . Make sure you have a kubeconfig file with admin permissions on the cluster you want to turn into Kamaji Admin Cluster.","title":"Access Admin cluster"},{"location":"guides/kamaji-deployment-guide/#install-datastore","text":"The Kamaji controller needs to access a multi-tenant datastore in order to save data of the tenants' clusters. The Helm Chart provides the installation of an unamanaged etcd . However, a managed etcd is highly recommended in production. The kamaji-etcd project provides a viable option to setup a manged multi-tenant etcd as 3 replicas StatefulSet with data persistence: helm repo add clastix https://clastix.github.io/charts helm repo update helm install etcd clastix/kamaji-etcd -n kamaji-system --create-namespace Optionally, Kamaji offers the possibility of using a different storage system for the tenants' clusters, as MySQL or PostgreSQL compatible database, thanks to the native kine integration.","title":"Install datastore"},{"location":"guides/kamaji-deployment-guide/#install-kamaji-controller","text":"There are multiple ways to deploy Kamaji, including a single YAML file and the Helm Chart . Install with helm using an unmanaged etcd as datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace Alternatively, if you opted for a managed etcd datastore: helm repo add clastix https://clastix.github.io/charts helm repo update helm install kamaji clastix/kamaji -n kamaji-system --create-namespace --set etcd.deploy=false Congratulations! You just turned your Kubernetes cluster into a Kamaji cluster capable to run multiple Tenant Control Planes.","title":"Install Kamaji Controller"},{"location":"guides/kamaji-deployment-guide/#create-tenant-cluster","text":"","title":"Create Tenant Cluster"},{"location":"guides/kamaji-deployment-guide/#tenant-control-plane","text":"A tenant control plane of example looks like: cat > ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml <<EOF apiVersion: kamaji.clastix.io/v1alpha1 kind: TenantControlPlane metadata: name: ${TENANT_NAME} namespace: ${TENANT_NAMESPACE} spec: controlPlane: deployment: replicas: 3 additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} extraArgs: apiServer: [] controllerManager: [] scheduler: [] resources: apiServer: requests: cpu: 250m memory: 512Mi limits: {} controllerManager: requests: cpu: 125m memory: 256Mi limits: {} scheduler: requests: cpu: 125m memory: 256Mi limits: {} service: additionalMetadata: labels: tenant.clastix.io: ${TENANT_NAME} serviceType: LoadBalancer kubernetes: version: ${TENANT_VERSION} kubelet: cgroupfs: systemd admissionControllers: - ResourceQuota - LimitRanger networkProfile: port: ${TENANT_PORT} certSANs: - ${TENANT_NAME}.${TENANT_DOMAIN} serviceCidr: ${TENANT_SVC_CIDR} podCidr: ${TENANT_POD_CIDR} dnsServiceIPs: - ${TENANT_DNS_SERVICE} addons: coreDNS: {} kubeProxy: {} konnectivity: proxyPort: ${TENANT_PROXY_PORT} resources: requests: cpu: 100m memory: 128Mi limits: {} EOF kubectl -n ${TENANT_NAMESPACE} apply -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml After a few minutes, check the created resources in the tenants namespace and when ready it will look similar to the following: kubectl -n tenants get tcp,deploy,pods,svc NAME VERSION STATUS CONTROL-PLANE-ENDPOINT KUBECONFIG AGE tenantcontrolplane.kamaji.clastix.io/tenant-00 v1.23.1 Ready 192.168.32.240:6443 tenant-00-admin-kubeconfig 2m20s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/tenant-00 3/3 3 3 118s NAME READY STATUS RESTARTS AGE pod/tenant-00-58847c8cdd-7hc4n 4/4 Running 0 82s pod/tenant-00-58847c8cdd-ft5xt 4/4 Running 0 82s pod/tenant-00-58847c8cdd-shc7t 4/4 Running 0 82s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/tenant-00 LoadBalancer 10.32.132.241 192.168.32.240 6443:32152/TCP,8132:32713/TCP 2m20s The regular Tenant Control Plane containers: kube-apiserver , kube-controller-manager , kube-scheduler are running unchanged in the tcp pods instead of dedicated machines and they are exposed through a service on the port 6443 of worker nodes in the Admin cluster. The LoadBalancer service type is used to expose the Tenant Control Plane. However, NodePort and ClusterIP with an Ingress Controller are still viable options, depending on the case. High Availability and rolling updates of the Tenant Control Plane are provided by the tcp Deployment and all the resources reconcilied by the Kamaji controller.","title":"Tenant Control Plane"},{"location":"guides/kamaji-deployment-guide/#working-with-tenant-control-plane","text":"Collect the external IP address of the tcp service: TENANT_ADDR=$(kubectl -n ${TENANT_NAMESPACE} get svc ${TENANT_NAME} -o json | jq -r .\"spec.loadBalancerIP\") and check it out: curl -k https://${TENANT_ADDR}:${TENANT_PORT}/healthz curl -k https://${TENANT_ADDR}:${TENANT_PORT}/version The kubeconfig required to access the Tenant Control Plane is stored in a secret: kubectl get secrets -n ${TENANT_NAMESPACE} ${TENANT_NAME}-admin-kubeconfig -o json \\ | jq -r '.data[\"admin.conf\"]' \\ | base64 --decode \\ > ${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig and let's check it out: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig cluster-info Kubernetes control plane is running at https://192.168.32.240:6443 CoreDNS is running at https://192.168.32.240:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Check out how the Tenant control Plane advertises itself to workloads: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get svc NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 6m kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get ep NAME ENDPOINTS AGE kubernetes 192.168.32.240:6443 18m And make sure it is ${TENANT_ADDR}:${TENANT_PORT} .","title":"Working with Tenant Control Plane"},{"location":"guides/kamaji-deployment-guide/#preparing-worker-nodes-to-join","text":"Currently Kamaji does not provide any helper for creation of tenant worker nodes. You should get a set of machines from your infrastructure provider, turn them into worker nodes, and then join to the tenant control plane with the kubeadm . In the future, we'll provide integration with Cluster APIs and other tools, as for example, Terraform. You can use the provided helper script nodes-prerequisites.sh , in order to install the dependencies on all the worker nodes: Install containerd as container runtime Install crictl , the command line for working with containerd Install kubectl , kubelet , and kubeadm in the desired version Warning: the script assumes all worker nodes are running Ubuntu 20.04 . Make sure to adapt the script if you're using a different distribution. Run the script: HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) ./nodes-prerequisites.sh ${TENANT_VERSION:1} ${HOSTS[@]}","title":"Preparing Worker Nodes to join"},{"location":"guides/kamaji-deployment-guide/#join-command","text":"The current approach for joining nodes is to use kubeadm and therefore, we will create a bootstrap token to perform the action. In order to facilitate the step, we will store the entire command of joining in a variable: JOIN_CMD=$(echo \"sudo \")$(kubeadm --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig token create --print-join-command)","title":"Join Command"},{"location":"guides/kamaji-deployment-guide/#adding-worker-nodes","text":"A bash loop will be used to join all the available nodes. HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) for i in \"${!HOSTS[@]}\"; do HOST=${HOSTS[$i]} ssh ${USER}@${HOST} -t ${JOIN_CMD}; done Checking the nodes: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-worker-00 NotReady <none> 25s v1.25.0 tenant-00-worker-01 NotReady <none> 17s v1.25.0 tenant-00-worker-02 NotReady <none> 9s v1.25.0 The cluster needs a CNI plugin to get the nodes ready. In this guide, we are going to install calico , but feel free to use one of your taste. Download the latest stable Calico manifest: curl https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml -O Before to apply the Calico manifest, you can customize it as necessary according to your preferences. Apply to the tenant cluster: kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig apply -f calico.yaml And after a while, nodes will be ready kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig get nodes NAME STATUS ROLES AGE VERSION tenant-00-worker-00 Ready <none> 2m48s v1.25.0 tenant-00-worker-01 Ready <none> 2m40s v1.25.0 tenant-00-worker-02 Ready <none> 2m32s v1.25.0","title":"Adding Worker Nodes"},{"location":"guides/kamaji-deployment-guide/#cleanup","text":"Remove the worker nodes joined the tenant control plane kubectl --kubeconfig=${TENANT_NAMESPACE}-${TENANT_NAME}.kubeconfig delete nodes --all For each worker node, login and clean it HOSTS=(${WORKER0} ${WORKER1} ${WORKER2}) for i in \"${!HOSTS[@]}\"; do HOST=${HOSTS[$i]} ssh ${USER}@${HOST} -t 'sudo kubeadm reset -f'; ssh ${USER}@${HOST} -t 'sudo rm -rf /etc/cni/net.d'; ssh ${USER}@${HOST} -t 'sudo systemctl reboot'; done Delete the tenant control plane from kamaji kubectl delete -f ${TENANT_NAMESPACE}-${TENANT_NAME}-tcp.yaml That's all folks!","title":"Cleanup"},{"location":"guides/mysql-datastore/","text":"MySQL as Kubernetes Storage Kamaji offers the possibility of having a different storage system than ETCD thanks to kine integration. One of the implementations is MySQL . A detailed guide for production setup will be released soon. Please refer to Getting Started Guide for a demo setup with KinD.","title":"MySQL as Kubernetes Storage"},{"location":"guides/mysql-datastore/#mysql-as-kubernetes-storage","text":"Kamaji offers the possibility of having a different storage system than ETCD thanks to kine integration. One of the implementations is MySQL . A detailed guide for production setup will be released soon. Please refer to Getting Started Guide for a demo setup with KinD.","title":"MySQL as Kubernetes Storage"},{"location":"guides/postgresql-datastore/","text":"PostgreSQL as Kubernetes Storage Kamaji offers the possibility of having a different storage system than etcd thanks to kine integration. One of the implementations is PostgreSQL . A detailed guide for production setup will be released soon. Please refer to Getting Started Guide for a demo setup with KinD.","title":"PostgreSQL as Kubernetes Storage"},{"location":"guides/postgresql-datastore/#postgresql-as-kubernetes-storage","text":"Kamaji offers the possibility of having a different storage system than etcd thanks to kine integration. One of the implementations is PostgreSQL . A detailed guide for production setup will be released soon. Please refer to Getting Started Guide for a demo setup with KinD.","title":"PostgreSQL as Kubernetes Storage"},{"location":"guides/upgrade/","text":"Tenant Cluster Upgrade The process of upgrading a \u201ctenant cluster\u201d consists in two steps: Upgrade the Tenant Control Plane Upgrade of Tenant Worker Nodes Upgrade of Tenant Control Plane You should patch the TenantControlPlane.spec.kubernetes.version custom resource with a new compatible value according to the Version Skew Policy . Note: during the upgrade, a new ReplicaSet of Tenant Control Plane pod will be created, so make sure you have at least two pods to avoid service disruption. Upgrade of Tenant Worker Nodes As currently Kamaji is not providing any helpers for Tenant Worker Nodes, you should make sure to upgrade them manually, for example, with the help of kubeadm . We have in roadmap, the Cluster APIs support so that you can upgrade \u201ctenant clusters\u201d in a fully declarative way.","title":"Tenant Cluster Upgrade"},{"location":"guides/upgrade/#tenant-cluster-upgrade","text":"The process of upgrading a \u201ctenant cluster\u201d consists in two steps: Upgrade the Tenant Control Plane Upgrade of Tenant Worker Nodes","title":"Tenant Cluster Upgrade"},{"location":"guides/upgrade/#upgrade-of-tenant-control-plane","text":"You should patch the TenantControlPlane.spec.kubernetes.version custom resource with a new compatible value according to the Version Skew Policy . Note: during the upgrade, a new ReplicaSet of Tenant Control Plane pod will be created, so make sure you have at least two pods to avoid service disruption.","title":"Upgrade of Tenant Control Plane"},{"location":"guides/upgrade/#upgrade-of-tenant-worker-nodes","text":"As currently Kamaji is not providing any helpers for Tenant Worker Nodes, you should make sure to upgrade them manually, for example, with the help of kubeadm . We have in roadmap, the Cluster APIs support so that you can upgrade \u201ctenant clusters\u201d in a fully declarative way.","title":"Upgrade of Tenant Worker Nodes"},{"location":"reference/","text":"References This section of the Kamaji documentation contains references to the project's specifications.","title":"References"},{"location":"reference/#references","text":"This section of the Kamaji documentation contains references to the project's specifications.","title":"References"},{"location":"reference/api/","text":"API Reference Packages: kamaji.clastix.io/v1alpha1 kamaji.clastix.io/v1alpha1 Resource Types: DataStore TenantControlPlane DataStore DataStore is the Schema for the datastores API. Name Type Description Required apiVersion string kamaji.clastix.io/v1alpha1 true kind string DataStore true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object DataStoreSpec defines the desired state of DataStore. false status object DataStoreStatus defines the observed state of DataStore. false DataStore.spec DataStoreSpec defines the desired state of DataStore. Name Type Description Required driver string The driver to use to connect to the shared datastore. true endpoints []string List of the endpoints to connect to the shared datastore. No need for protocol, just bare IP/FQDN and port. true tlsConfig object Defines the TLS/SSL configuration required to connect to the data store in a secure way. true basicAuth object In case of authentication enabled for the given data store, specifies the username and password pair. This value is optional. false DataStore.spec.tlsConfig Defines the TLS/SSL configuration required to connect to the data store in a secure way. Name Type Description Required certificateAuthority object Retrieve the Certificate Authority certificate and private key, such as bare content of the file, or a SecretReference. The key reference is required since etcd authentication is based on certificates, and Kamaji is responsible in creating this. true clientCertificate object Specifies the SSL/TLS key and private key pair used to connect to the data store. true DataStore.spec.tlsConfig.certificateAuthority Retrieve the Certificate Authority certificate and private key, such as bare content of the file, or a SecretReference. The key reference is required since etcd authentication is based on certificates, and Kamaji is responsible in creating this. Name Type Description Required certificate object true privateKey object false DataStore.spec.tlsConfig.certificateAuthority.certificate Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.tlsConfig.certificateAuthority.certificate.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.spec.tlsConfig.certificateAuthority.privateKey Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.tlsConfig.certificateAuthority.privateKey.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.spec.tlsConfig.clientCertificate Specifies the SSL/TLS key and private key pair used to connect to the data store. Name Type Description Required certificate object true privateKey object true DataStore.spec.tlsConfig.clientCertificate.certificate Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.tlsConfig.clientCertificate.certificate.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.spec.tlsConfig.clientCertificate.privateKey Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.tlsConfig.clientCertificate.privateKey.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.spec.basicAuth In case of authentication enabled for the given data store, specifies the username and password pair. This value is optional. Name Type Description Required password object true username object true DataStore.spec.basicAuth.password Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.basicAuth.password.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.spec.basicAuth.username Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false DataStore.spec.basicAuth.username.secretReference Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false DataStore.status DataStoreStatus defines the observed state of DataStore. Name Type Description Required usedBy []string List of the Tenant Control Planes, namespaced named, using this data store. false TenantControlPlane TenantControlPlane is the Schema for the tenantcontrolplanes API. Name Type Description Required apiVersion string kamaji.clastix.io/v1alpha1 true kind string TenantControlPlane true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object TenantControlPlaneSpec defines the desired state of TenantControlPlane. false status object TenantControlPlaneStatus defines the observed state of TenantControlPlane. false TenantControlPlane.spec TenantControlPlaneSpec defines the desired state of TenantControlPlane. Name Type Description Required controlPlane object ControlPlane defines how the Tenant Control Plane Kubernetes resources must be created in the Admin Cluster, such as the number of Pod replicas, the Service resource, or the Ingress. true kubernetes object Kubernetes specification for tenant control plane true addons object Addons contain which addons are enabled false dataStore string DataStore allows to specify a DataStore that should be used to store the Kubernetes data for the given Tenant Control Plane. This parameter is optional and acts as an override over the default one which is used by the Kamaji Operator. Migration from a different DataStore to another one is not yet supported and the reconciliation will be blocked. false networkProfile object NetworkProfile specifies how the network is false TenantControlPlane.spec.controlPlane ControlPlane defines how the Tenant Control Plane Kubernetes resources must be created in the Admin Cluster, such as the number of Pod replicas, the Service resource, or the Ingress. Name Type Description Required service object Defining the options for the Tenant Control Plane Service resource. true deployment object Defining the options for the deployed Tenant Control Plane as Deployment resource. false ingress object Defining the options for an Optional Ingress which will expose API Server of the Tenant Control Plane false TenantControlPlane.spec.controlPlane.service Defining the options for the Tenant Control Plane Service resource. Name Type Description Required serviceType enum ServiceType allows specifying how to expose the Tenant Control Plane. Enum : ClusterIP, NodePort, LoadBalancer true additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false TenantControlPlane.spec.controlPlane.service.additionalMetadata AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false TenantControlPlane.spec.controlPlane.deployment Defining the options for the deployed Tenant Control Plane as Deployment resource. Name Type Description Required additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false extraArgs object ExtraArgs allows adding additional arguments to the Control Plane components, such as kube-apiserver, controller-manager, and scheduler. false replicas integer Format : int32 Default : 2 false resources object Resources defines the amount of memory and CPU to allocate to each component of the Control Plane (kube-apiserver, controller-manager, and scheduler). false topologySpreadConstraints []object TopologySpreadConstraints describes how the Tenant Control Plane pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. In case of nil underlying LabelSelector, the Kamaji one for the given Tenant Control Plane will be used. All topologySpreadConstraints are ANDed. false TenantControlPlane.spec.controlPlane.deployment.additionalMetadata AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false TenantControlPlane.spec.controlPlane.deployment.extraArgs ExtraArgs allows adding additional arguments to the Control Plane components, such as kube-apiserver, controller-manager, and scheduler. Name Type Description Required apiServer []string false controllerManager []string false kine []string Available only if Kamaji is running using Kine as backing storage. false scheduler []string false TenantControlPlane.spec.controlPlane.deployment.resources Resources defines the amount of memory and CPU to allocate to each component of the Control Plane (kube-apiserver, controller-manager, and scheduler). Name Type Description Required apiServer object ResourceRequirements describes the compute resource requirements. false controllerManager object ResourceRequirements describes the compute resource requirements. false scheduler object ResourceRequirements describes the compute resource requirements. false TenantControlPlane.spec.controlPlane.deployment.resources.apiServer ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false TenantControlPlane.spec.controlPlane.deployment.resources.controllerManager ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false TenantControlPlane.spec.controlPlane.deployment.resources.scheduler ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index] TopologySpreadConstraint specifies how to spread matching pods among the given topology. Name Type Description Required maxSkew integer MaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | | P P | P P | P | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed. Format : int32 true topologyKey string TopologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the same topology. We consider each as a \"bucket\", and try to put balanced number of pods into each bucket. We define a domain as a particular instance of a topology. Also, we define an eligible domain as a domain whose nodes meet the requirements of nodeAffinityPolicy and nodeTaintsPolicy. e.g. If TopologyKey is \"kubernetes.io/hostname\", each Node is a domain of that topology. And, if TopologyKey is \"topology.kubernetes.io/zone\", each zone is a domain of that topology. It's a required field. true whenUnsatisfiable string WhenUnsatisfiable indicates how to deal with a pod if it doesn't satisfy the spread constraint. - DoNotSchedule (default) tells the scheduler not to schedule it. - ScheduleAnyway tells the scheduler to schedule the pod in any location, but giving higher precedence to topologies that would help reduce the skew. A constraint is considered \"Unsatisfiable\" for an incoming pod if and only if every possible node assignment for that pod would violate \"MaxSkew\" on some topology. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 3/1/1: | zone1 | zone2 | zone3 | | P P P | P | P | If WhenUnsatisfiable is set to DoNotSchedule, incoming pod can only be scheduled to zone2(zone3) to become 3/2/1(3/1/2) as ActualSkew(2-1) on zone2(zone3) satisfies MaxSkew(1). In other words, the cluster can still be imbalanced, but scheduler won't make it *more* imbalanced. It's a required field. true labelSelector object LabelSelector is used to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain. false matchLabelKeys []string MatchLabelKeys is a set of pod label keys to select the pods over which spreading will be calculated. The keys are used to lookup values from the incoming pod labels, those key-value labels are ANDed with labelSelector to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the incoming pod labels will be ignored. A null or empty list means only match against labelSelector. false minDomains integer MinDomains indicates a minimum number of eligible domains. When the number of eligible domains with matching topology keys is less than minDomains, Pod Topology Spread treats \"global minimum\" as 0, and then the calculation of Skew is performed. And when the number of eligible domains with matching topology keys equals or greater than minDomains, this value has no effect on scheduling. As a result, when the number of eligible domains is less than minDomains, scheduler won't schedule more than maxSkew Pods to those domains. If value is nil, the constraint behaves as if MinDomains is equal to 1. Valid values are integers greater than 0. When value is not nil, WhenUnsatisfiable must be DoNotSchedule. For example, in a 3-zone cluster, MaxSkew is set to 2, MinDomains is set to 5 and pods with the same labelSelector spread as 2/2/2: | zone1 | zone2 | zone3 | | P P | P P | P P | The number of domains is less than 5(MinDomains), so \"global minimum\" is treated as 0. In this situation, new pod with the same labelSelector cannot be scheduled, because computed skew will be 3(3 - 0) if new Pod is scheduled to any of the three zones, it will violate MaxSkew. This is a beta field and requires the MinDomainsInPodTopologySpread feature gate to be enabled (enabled by default). Format : int32 false nodeAffinityPolicy string NodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew. Options are: - Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations. - Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations. If this value is nil, the behavior is equivalent to the Honor policy. This is a alpha-level feature enabled by the NodeInclusionPolicyInPodTopologySpread feature flag. false nodeTaintsPolicy string NodeTaintsPolicy indicates how we will treat node taints when calculating pod topology spread skew. Options are: - Honor: nodes without taints, along with tainted nodes for which the incoming pod has a toleration, are included. - Ignore: node taints are ignored. All nodes are included. If this value is nil, the behavior is equivalent to the Ignore policy. This is a alpha-level feature enabled by the NodeInclusionPolicyInPodTopologySpread feature flag. false TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index].labelSelector LabelSelector is used to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index].labelSelector.matchExpressions[index] A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false TenantControlPlane.spec.controlPlane.ingress Defining the options for an Optional Ingress which will expose API Server of the Tenant Control Plane Name Type Description Required additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false hostname string Hostname is an optional field which will be used as Ingress's Host. If it is not defined, Ingress's host will be \" . . \", where domain is specified under NetworkProfileSpec false ingressClassName string false TenantControlPlane.spec.controlPlane.ingress.additionalMetadata AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false TenantControlPlane.spec.kubernetes Kubernetes specification for tenant control plane Name Type Description Required kubelet object true version string Kubernetes Version for the tenant control plane true admissionControllers []enum List of enabled Admission Controllers for the Tenant cluster. Full reference available here: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers Default : [CertificateApproval CertificateSigning CertificateSubjectRestriction DefaultIngressClass DefaultStorageClass DefaultTolerationSeconds LimitRanger MutatingAdmissionWebhook NamespaceLifecycle PersistentVolumeClaimResize Priority ResourceQuota RuntimeClass ServiceAccount StorageObjectInUseProtection TaintNodesByCondition ValidatingAdmissionWebhook] false TenantControlPlane.spec.kubernetes.kubelet Name Type Description Required cgroupfs enum CGroupFS defines the cgroup driver for Kubelet https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/ Enum : systemd, cgroupfs false TenantControlPlane.spec.addons Addons contain which addons are enabled Name Type Description Required coreDNS object Enables the DNS addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to `coredns`. false konnectivity object Enables the Konnectivity addon in the Tenant Cluster, required if the worker nodes are in a different network. false kubeProxy object Enables the kube-proxy addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to `kube-proxy`. false TenantControlPlane.spec.addons.coreDNS Enables the DNS addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to coredns . Name Type Description Required imageRepository string ImageRepository sets the container registry to pull images from. if not set, the default ImageRepository will be used instead. false imageTag string ImageTag allows to specify a tag for the image. In case this value is set, kubeadm does not change automatically the version of the above components during upgrades. false TenantControlPlane.spec.addons.konnectivity Enables the Konnectivity addon in the Tenant Cluster, required if the worker nodes are in a different network. Name Type Description Required proxyPort integer Port of Konnectivity proxy server. Format : int32 true agentImage string AgentImage defines the container image for Konnectivity's agent. Default : registry.k8s.io/kas-network-proxy/proxy-agent false resources object Resources define the amount of CPU and memory to allocate to the Konnectivity server. false serverImage string ServerImage defines the container image for Konnectivity's server. Default : registry.k8s.io/kas-network-proxy/proxy-server false version string Version for Konnectivity server and agent. Default : v0.0.32 false TenantControlPlane.spec.addons.konnectivity.resources Resources define the amount of CPU and memory to allocate to the Konnectivity server. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false TenantControlPlane.spec.addons.kubeProxy Enables the kube-proxy addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to kube-proxy . Name Type Description Required imageRepository string ImageRepository sets the container registry to pull images from. if not set, the default ImageRepository will be used instead. false imageTag string ImageTag allows to specify a tag for the image. In case this value is set, kubeadm does not change automatically the version of the above components during upgrades. false TenantControlPlane.spec.networkProfile NetworkProfile specifies how the network is Name Type Description Required address string Address where API server of will be exposed. In case of LoadBalancer Service, this can be empty in order to use the exposed IP provided by the cloud controller manager. false allowAddressAsExternalIP boolean AllowAddressAsExternalIP will include tenantControlPlane.Spec.NetworkProfile.Address in the section of ExternalIPs of the Kubernetes Service (only ClusterIP or NodePort) false certSANs []string CertSANs sets extra Subject Alternative Names (SANs) for the API Server signing certificate. Use this field to add additional hostnames when exposing the Tenant Control Plane with third solutions. false dnsServiceIPs []string Default : [10.96.0.10] false podCidr string CIDR for Kubernetes Pods Default : 10.244.0.0/16 false port integer Port where API server of will be exposed Format : int32 Default : 6443 false serviceCidr string Kubernetes Service Default : 10.96.0.0/16 false TenantControlPlane.status TenantControlPlaneStatus defines the observed state of TenantControlPlane. Name Type Description Required addons object Addons contains the status of the different Addons false certificates object Certificates contains information about the different certificates that are necessary to run a kubernetes control plane false controlPlaneEndpoint string ControlPlaneEndpoint contains the status of the kubernetes control plane false kubeadmPhase object KubeadmPhase contains the status of the kubeadm phases action false kubeadmconfig object KubeadmConfig contains the status of the configuration required by kubeadm false kubeconfig object KubeConfig contains information about the kubenconfigs that control plane pieces need false kubernetesResources object Kubernetes contains information about the reconciliation of the required Kubernetes resources deployed in the admin cluster false storage object Storage Status contains information about Kubernetes storage system false TenantControlPlane.status.addons Addons contains the status of the different Addons Name Type Description Required coreDNS object AddonStatus defines the observed state of an Addon. false konnectivity object KonnectivityStatus defines the status of Konnectivity as Addon. false kubeProxy object AddonStatus defines the observed state of an Addon. false TenantControlPlane.status.addons.coreDNS AddonStatus defines the observed state of an Addon. Name Type Description Required enabled boolean true checksum string false lastUpdate string Format : date-time false TenantControlPlane.status.addons.konnectivity KonnectivityStatus defines the status of Konnectivity as Addon. Name Type Description Required enabled boolean true agent object false certificate object CertificatePrivateKeyPairStatus defines the status. false clusterrolebinding object false configMap object false kubeconfig object KubeconfigStatus contains information about the generated kubeconfig. false sa object false service object KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. false TenantControlPlane.status.addons.konnectivity.agent Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false TenantControlPlane.status.addons.konnectivity.certificate CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.addons.konnectivity.clusterrolebinding Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false TenantControlPlane.status.addons.konnectivity.configMap Name Type Description Required checksum string false name string false TenantControlPlane.status.addons.konnectivity.kubeconfig KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.addons.konnectivity.sa Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false TenantControlPlane.status.addons.konnectivity.service KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. Name Type Description Required name string The name of the Service for the given cluster. true namespace string The namespace which the Service for the given cluster is deployed. true port integer The port where the service is running Format : int32 true conditions []object Current service state false loadBalancer object LoadBalancer contains the current status of the load-balancer, if one is present. false TenantControlPlane.status.addons.konnectivity.service.conditions[index] Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions. For example, type FooStatus struct{ // Represents the observations of a foo's current state. // Known .status.conditions.type are: \"Available\", \"Progressing\", and \"Degraded\" // +patchMergeKey=type // +patchStrategy=merge // +listType=map // +listMapKey=type Conditions []metav1.Condition json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,1,rep,name=conditions\" // other fields } Name Type Description Required lastTransitionTime string lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format : date-time true message string message is a human readable message indicating details about the transition. This may be an empty string. true reason string reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum status of the condition, one of True, False, Unknown. Enum : True, False, Unknown true type string type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) true observedGeneration integer observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format : int64 Minimum : 0 false TenantControlPlane.status.addons.konnectivity.service.loadBalancer LoadBalancer contains the current status of the load-balancer, if one is present. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false TenantControlPlane.status.addons.konnectivity.service.loadBalancer.ingress[index] LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false TenantControlPlane.status.addons.konnectivity.service.loadBalancer.ingress[index].ports[index] Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false TenantControlPlane.status.addons.kubeProxy AddonStatus defines the observed state of an Addon. Name Type Description Required enabled boolean true checksum string false lastUpdate string Format : date-time false TenantControlPlane.status.certificates Certificates contains information about the different certificates that are necessary to run a kubernetes control plane Name Type Description Required apiServer object CertificatePrivateKeyPairStatus defines the status. false apiServerKubeletClient object CertificatePrivateKeyPairStatus defines the status. false ca object CertificatePrivateKeyPairStatus defines the status. false etcd object ETCDCertificatesStatus defines the observed state of ETCD Certificate for API server. false frontProxyCA object CertificatePrivateKeyPairStatus defines the status. false frontProxyClient object CertificatePrivateKeyPairStatus defines the status. false sa object PublicKeyPrivateKeyPairStatus defines the status. false TenantControlPlane.status.certificates.apiServer CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.apiServerKubeletClient CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.ca CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.etcd ETCDCertificatesStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required apiServer object APIServerCertificatesStatus defines the observed state of ETCD Certificate for API server. false ca object ETCDCertificateStatus defines the observed state of ETCD Certificate for API server. false TenantControlPlane.status.certificates.etcd.apiServer APIServerCertificatesStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.etcd.ca ETCDCertificateStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.frontProxyCA CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.frontProxyClient CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.certificates.sa PublicKeyPrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.kubeadmPhase KubeadmPhase contains the status of the kubeadm phases action Name Type Description Required bootstrapToken object KubeadmPhaseStatus contains the status of a kubeadm phase action. true uploadConfigKubeadm object KubeadmPhaseStatus contains the status of a kubeadm phase action. true uploadConfigKubelet object KubeadmPhaseStatus contains the status of a kubeadm phase action. true TenantControlPlane.status.kubeadmPhase.bootstrapToken KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false TenantControlPlane.status.kubeadmPhase.uploadConfigKubeadm KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false TenantControlPlane.status.kubeadmPhase.uploadConfigKubelet KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false TenantControlPlane.status.kubeadmconfig KubeadmConfig contains the status of the configuration required by kubeadm Name Type Description Required checksum string Checksum of the kubeadm configuration to detect changes false configmapName string false lastUpdate string Format : date-time false TenantControlPlane.status.kubeconfig KubeConfig contains information about the kubenconfigs that control plane pieces need Name Type Description Required admin object KubeconfigStatus contains information about the generated kubeconfig. false controllerManager object KubeconfigStatus contains information about the generated kubeconfig. false scheduler object KubeconfigStatus contains information about the generated kubeconfig. false TenantControlPlane.status.kubeconfig.admin KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.kubeconfig.controllerManager KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.kubeconfig.scheduler KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.kubernetesResources Kubernetes contains information about the reconciliation of the required Kubernetes resources deployed in the admin cluster Name Type Description Required deployment object KubernetesDeploymentStatus defines the status for the Tenant Control Plane Deployment in the management cluster. false ingress object KubernetesIngressStatus defines the status for the Tenant Control Plane Ingress in the management cluster. false service object KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. false version object KubernetesVersion contains the information regarding the running Kubernetes version, and its upgrade status. false TenantControlPlane.status.kubernetesResources.deployment KubernetesDeploymentStatus defines the status for the Tenant Control Plane Deployment in the management cluster. Name Type Description Required name string The name of the Deployment for the given cluster. true namespace string The namespace which the Deployment for the given cluster is deployed. true selector string Selector is the label selector used to group the Tenant Control Plane Pods used by the scale subresource. true availableReplicas integer Total number of available pods (ready for at least minReadySeconds) targeted by this deployment. Format : int32 false collisionCount integer Count of hash collisions for the Deployment. The Deployment controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ReplicaSet. Format : int32 false conditions []object Represents the latest available observations of a deployment's current state. false lastUpdate string Last time when deployment was updated Format : date-time false observedGeneration integer The generation observed by the deployment controller. Format : int64 false readyReplicas integer readyReplicas is the number of pods targeted by this Deployment with a Ready Condition. Format : int32 false replicas integer Total number of non-terminated pods targeted by this deployment (their labels match the selector). Format : int32 false unavailableReplicas integer Total number of unavailable pods targeted by this deployment. This is the total number of pods that are still required for the deployment to have 100% available capacity. They may either be pods that are running but not yet available or pods that still have not been created. Format : int32 false updatedReplicas integer Total number of non-terminated pods targeted by this deployment that have the desired template spec. Format : int32 false TenantControlPlane.status.kubernetesResources.deployment.conditions[index] DeploymentCondition describes the state of a deployment at a certain point. Name Type Description Required status string Status of the condition, one of True, False, Unknown. true type string Type of deployment condition. true lastTransitionTime string Last time the condition transitioned from one status to another. Format : date-time false lastUpdateTime string The last time this condition was updated. Format : date-time false message string A human readable message indicating details about the transition. false reason string The reason for the condition's last transition. false TenantControlPlane.status.kubernetesResources.ingress KubernetesIngressStatus defines the status for the Tenant Control Plane Ingress in the management cluster. Name Type Description Required name string The name of the Ingress for the given cluster. true namespace string The namespace which the Ingress for the given cluster is deployed. true loadBalancer object LoadBalancer contains the current status of the load-balancer. false TenantControlPlane.status.kubernetesResources.ingress.loadBalancer LoadBalancer contains the current status of the load-balancer. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false TenantControlPlane.status.kubernetesResources.ingress.loadBalancer.ingress[index] LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false TenantControlPlane.status.kubernetesResources.ingress.loadBalancer.ingress[index].ports[index] Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false TenantControlPlane.status.kubernetesResources.service KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. Name Type Description Required name string The name of the Service for the given cluster. true namespace string The namespace which the Service for the given cluster is deployed. true port integer The port where the service is running Format : int32 true conditions []object Current service state false loadBalancer object LoadBalancer contains the current status of the load-balancer, if one is present. false TenantControlPlane.status.kubernetesResources.service.conditions[index] Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions. For example, type FooStatus struct{ // Represents the observations of a foo's current state. // Known .status.conditions.type are: \"Available\", \"Progressing\", and \"Degraded\" // +patchMergeKey=type // +patchStrategy=merge // +listType=map // +listMapKey=type Conditions []metav1.Condition json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,1,rep,name=conditions\" // other fields } Name Type Description Required lastTransitionTime string lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format : date-time true message string message is a human readable message indicating details about the transition. This may be an empty string. true reason string reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum status of the condition, one of True, False, Unknown. Enum : True, False, Unknown true type string type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) true observedGeneration integer observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format : int64 Minimum : 0 false TenantControlPlane.status.kubernetesResources.service.loadBalancer LoadBalancer contains the current status of the load-balancer, if one is present. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false TenantControlPlane.status.kubernetesResources.service.loadBalancer.ingress[index] LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false TenantControlPlane.status.kubernetesResources.service.loadBalancer.ingress[index].ports[index] Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false TenantControlPlane.status.kubernetesResources.version KubernetesVersion contains the information regarding the running Kubernetes version, and its upgrade status. Name Type Description Required status enum Status returns the current status of the Kubernetes version, such as its provisioning state, or completed upgrade. Enum : Provisioning, Upgrading, Ready, NotReady Default : Provisioning false version string Version is the running Kubernetes version of the Tenant Control Plane. false TenantControlPlane.status.storage Storage Status contains information about Kubernetes storage system Name Type Description Required certificate object false config object false dataStoreName string false driver string false setup object false TenantControlPlane.status.storage.certificate Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false TenantControlPlane.status.storage.config Name Type Description Required checksum string false secretName string false TenantControlPlane.status.storage.setup Name Type Description Required checksum string false lastUpdate string Format : date-time false schema string false user string false","title":"API Reference"},{"location":"reference/api/#api-reference","text":"Packages: kamaji.clastix.io/v1alpha1","title":"API Reference"},{"location":"reference/api/#kamajiclastixiov1alpha1","text":"Resource Types: DataStore TenantControlPlane","title":"kamaji.clastix.io/v1alpha1"},{"location":"reference/api/#datastore","text":"DataStore is the Schema for the datastores API. Name Type Description Required apiVersion string kamaji.clastix.io/v1alpha1 true kind string DataStore true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object DataStoreSpec defines the desired state of DataStore. false status object DataStoreStatus defines the observed state of DataStore. false","title":"DataStore"},{"location":"reference/api/#datastorespec","text":"DataStoreSpec defines the desired state of DataStore. Name Type Description Required driver string The driver to use to connect to the shared datastore. true endpoints []string List of the endpoints to connect to the shared datastore. No need for protocol, just bare IP/FQDN and port. true tlsConfig object Defines the TLS/SSL configuration required to connect to the data store in a secure way. true basicAuth object In case of authentication enabled for the given data store, specifies the username and password pair. This value is optional. false","title":"DataStore.spec"},{"location":"reference/api/#datastorespectlsconfig","text":"Defines the TLS/SSL configuration required to connect to the data store in a secure way. Name Type Description Required certificateAuthority object Retrieve the Certificate Authority certificate and private key, such as bare content of the file, or a SecretReference. The key reference is required since etcd authentication is based on certificates, and Kamaji is responsible in creating this. true clientCertificate object Specifies the SSL/TLS key and private key pair used to connect to the data store. true","title":"DataStore.spec.tlsConfig"},{"location":"reference/api/#datastorespectlsconfigcertificateauthority","text":"Retrieve the Certificate Authority certificate and private key, such as bare content of the file, or a SecretReference. The key reference is required since etcd authentication is based on certificates, and Kamaji is responsible in creating this. Name Type Description Required certificate object true privateKey object false","title":"DataStore.spec.tlsConfig.certificateAuthority"},{"location":"reference/api/#datastorespectlsconfigcertificateauthoritycertificate","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.tlsConfig.certificateAuthority.certificate"},{"location":"reference/api/#datastorespectlsconfigcertificateauthoritycertificatesecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.tlsConfig.certificateAuthority.certificate.secretReference"},{"location":"reference/api/#datastorespectlsconfigcertificateauthorityprivatekey","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.tlsConfig.certificateAuthority.privateKey"},{"location":"reference/api/#datastorespectlsconfigcertificateauthorityprivatekeysecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.tlsConfig.certificateAuthority.privateKey.secretReference"},{"location":"reference/api/#datastorespectlsconfigclientcertificate","text":"Specifies the SSL/TLS key and private key pair used to connect to the data store. Name Type Description Required certificate object true privateKey object true","title":"DataStore.spec.tlsConfig.clientCertificate"},{"location":"reference/api/#datastorespectlsconfigclientcertificatecertificate","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.tlsConfig.clientCertificate.certificate"},{"location":"reference/api/#datastorespectlsconfigclientcertificatecertificatesecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.tlsConfig.clientCertificate.certificate.secretReference"},{"location":"reference/api/#datastorespectlsconfigclientcertificateprivatekey","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.tlsConfig.clientCertificate.privateKey"},{"location":"reference/api/#datastorespectlsconfigclientcertificateprivatekeysecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.tlsConfig.clientCertificate.privateKey.secretReference"},{"location":"reference/api/#datastorespecbasicauth","text":"In case of authentication enabled for the given data store, specifies the username and password pair. This value is optional. Name Type Description Required password object true username object true","title":"DataStore.spec.basicAuth"},{"location":"reference/api/#datastorespecbasicauthpassword","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.basicAuth.password"},{"location":"reference/api/#datastorespecbasicauthpasswordsecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.basicAuth.password.secretReference"},{"location":"reference/api/#datastorespecbasicauthusername","text":"Name Type Description Required content string Bare content of the file, base64 encoded. It has precedence over the SecretReference value. Format : byte false secretReference object false","title":"DataStore.spec.basicAuth.username"},{"location":"reference/api/#datastorespecbasicauthusernamesecretreference","text":"Name Type Description Required keyPath string Name of the key for the given Secret reference where the content is stored. This value is mandatory. true name string name is unique within a namespace to reference a secret resource. false namespace string namespace defines the space within which the secret name must be unique. false","title":"DataStore.spec.basicAuth.username.secretReference"},{"location":"reference/api/#datastorestatus","text":"DataStoreStatus defines the observed state of DataStore. Name Type Description Required usedBy []string List of the Tenant Control Planes, namespaced named, using this data store. false","title":"DataStore.status"},{"location":"reference/api/#tenantcontrolplane","text":"TenantControlPlane is the Schema for the tenantcontrolplanes API. Name Type Description Required apiVersion string kamaji.clastix.io/v1alpha1 true kind string TenantControlPlane true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object TenantControlPlaneSpec defines the desired state of TenantControlPlane. false status object TenantControlPlaneStatus defines the observed state of TenantControlPlane. false","title":"TenantControlPlane"},{"location":"reference/api/#tenantcontrolplanespec","text":"TenantControlPlaneSpec defines the desired state of TenantControlPlane. Name Type Description Required controlPlane object ControlPlane defines how the Tenant Control Plane Kubernetes resources must be created in the Admin Cluster, such as the number of Pod replicas, the Service resource, or the Ingress. true kubernetes object Kubernetes specification for tenant control plane true addons object Addons contain which addons are enabled false dataStore string DataStore allows to specify a DataStore that should be used to store the Kubernetes data for the given Tenant Control Plane. This parameter is optional and acts as an override over the default one which is used by the Kamaji Operator. Migration from a different DataStore to another one is not yet supported and the reconciliation will be blocked. false networkProfile object NetworkProfile specifies how the network is false","title":"TenantControlPlane.spec"},{"location":"reference/api/#tenantcontrolplanespeccontrolplane","text":"ControlPlane defines how the Tenant Control Plane Kubernetes resources must be created in the Admin Cluster, such as the number of Pod replicas, the Service resource, or the Ingress. Name Type Description Required service object Defining the options for the Tenant Control Plane Service resource. true deployment object Defining the options for the deployed Tenant Control Plane as Deployment resource. false ingress object Defining the options for an Optional Ingress which will expose API Server of the Tenant Control Plane false","title":"TenantControlPlane.spec.controlPlane"},{"location":"reference/api/#tenantcontrolplanespeccontrolplaneservice","text":"Defining the options for the Tenant Control Plane Service resource. Name Type Description Required serviceType enum ServiceType allows specifying how to expose the Tenant Control Plane. Enum : ClusterIP, NodePort, LoadBalancer true additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false","title":"TenantControlPlane.spec.controlPlane.service"},{"location":"reference/api/#tenantcontrolplanespeccontrolplaneserviceadditionalmetadata","text":"AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false","title":"TenantControlPlane.spec.controlPlane.service.additionalMetadata"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeployment","text":"Defining the options for the deployed Tenant Control Plane as Deployment resource. Name Type Description Required additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false extraArgs object ExtraArgs allows adding additional arguments to the Control Plane components, such as kube-apiserver, controller-manager, and scheduler. false replicas integer Format : int32 Default : 2 false resources object Resources defines the amount of memory and CPU to allocate to each component of the Control Plane (kube-apiserver, controller-manager, and scheduler). false topologySpreadConstraints []object TopologySpreadConstraints describes how the Tenant Control Plane pods ought to spread across topology domains. Scheduler will schedule pods in a way which abides by the constraints. In case of nil underlying LabelSelector, the Kamaji one for the given Tenant Control Plane will be used. All topologySpreadConstraints are ANDed. false","title":"TenantControlPlane.spec.controlPlane.deployment"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentadditionalmetadata","text":"AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false","title":"TenantControlPlane.spec.controlPlane.deployment.additionalMetadata"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentextraargs","text":"ExtraArgs allows adding additional arguments to the Control Plane components, such as kube-apiserver, controller-manager, and scheduler. Name Type Description Required apiServer []string false controllerManager []string false kine []string Available only if Kamaji is running using Kine as backing storage. false scheduler []string false","title":"TenantControlPlane.spec.controlPlane.deployment.extraArgs"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentresources","text":"Resources defines the amount of memory and CPU to allocate to each component of the Control Plane (kube-apiserver, controller-manager, and scheduler). Name Type Description Required apiServer object ResourceRequirements describes the compute resource requirements. false controllerManager object ResourceRequirements describes the compute resource requirements. false scheduler object ResourceRequirements describes the compute resource requirements. false","title":"TenantControlPlane.spec.controlPlane.deployment.resources"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentresourcesapiserver","text":"ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false","title":"TenantControlPlane.spec.controlPlane.deployment.resources.apiServer"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentresourcescontrollermanager","text":"ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false","title":"TenantControlPlane.spec.controlPlane.deployment.resources.controllerManager"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymentresourcesscheduler","text":"ResourceRequirements describes the compute resource requirements. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false","title":"TenantControlPlane.spec.controlPlane.deployment.resources.scheduler"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymenttopologyspreadconstraintsindex","text":"TopologySpreadConstraint specifies how to spread matching pods among the given topology. Name Type Description Required maxSkew integer MaxSkew describes the degree to which pods may be unevenly distributed. When `whenUnsatisfiable=DoNotSchedule`, it is the maximum permitted difference between the number of matching pods in the target topology and the global minimum. The global minimum is the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 2/2/1: In this case, the global minimum is 1. | zone1 | zone2 | zone3 | | P P | P P | P | - if MaxSkew is 1, incoming pod can only be scheduled to zone3 to become 2/2/2; scheduling it onto zone1(zone2) would make the ActualSkew(3-1) on zone1(zone2) violate MaxSkew(1). - if MaxSkew is 2, incoming pod can be scheduled onto any zone. When `whenUnsatisfiable=ScheduleAnyway`, it is used to give higher precedence to topologies that satisfy it. It's a required field. Default value is 1 and 0 is not allowed. Format : int32 true topologyKey string TopologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the same topology. We consider each as a \"bucket\", and try to put balanced number of pods into each bucket. We define a domain as a particular instance of a topology. Also, we define an eligible domain as a domain whose nodes meet the requirements of nodeAffinityPolicy and nodeTaintsPolicy. e.g. If TopologyKey is \"kubernetes.io/hostname\", each Node is a domain of that topology. And, if TopologyKey is \"topology.kubernetes.io/zone\", each zone is a domain of that topology. It's a required field. true whenUnsatisfiable string WhenUnsatisfiable indicates how to deal with a pod if it doesn't satisfy the spread constraint. - DoNotSchedule (default) tells the scheduler not to schedule it. - ScheduleAnyway tells the scheduler to schedule the pod in any location, but giving higher precedence to topologies that would help reduce the skew. A constraint is considered \"Unsatisfiable\" for an incoming pod if and only if every possible node assignment for that pod would violate \"MaxSkew\" on some topology. For example, in a 3-zone cluster, MaxSkew is set to 1, and pods with the same labelSelector spread as 3/1/1: | zone1 | zone2 | zone3 | | P P P | P | P | If WhenUnsatisfiable is set to DoNotSchedule, incoming pod can only be scheduled to zone2(zone3) to become 3/2/1(3/1/2) as ActualSkew(2-1) on zone2(zone3) satisfies MaxSkew(1). In other words, the cluster can still be imbalanced, but scheduler won't make it *more* imbalanced. It's a required field. true labelSelector object LabelSelector is used to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain. false matchLabelKeys []string MatchLabelKeys is a set of pod label keys to select the pods over which spreading will be calculated. The keys are used to lookup values from the incoming pod labels, those key-value labels are ANDed with labelSelector to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the incoming pod labels will be ignored. A null or empty list means only match against labelSelector. false minDomains integer MinDomains indicates a minimum number of eligible domains. When the number of eligible domains with matching topology keys is less than minDomains, Pod Topology Spread treats \"global minimum\" as 0, and then the calculation of Skew is performed. And when the number of eligible domains with matching topology keys equals or greater than minDomains, this value has no effect on scheduling. As a result, when the number of eligible domains is less than minDomains, scheduler won't schedule more than maxSkew Pods to those domains. If value is nil, the constraint behaves as if MinDomains is equal to 1. Valid values are integers greater than 0. When value is not nil, WhenUnsatisfiable must be DoNotSchedule. For example, in a 3-zone cluster, MaxSkew is set to 2, MinDomains is set to 5 and pods with the same labelSelector spread as 2/2/2: | zone1 | zone2 | zone3 | | P P | P P | P P | The number of domains is less than 5(MinDomains), so \"global minimum\" is treated as 0. In this situation, new pod with the same labelSelector cannot be scheduled, because computed skew will be 3(3 - 0) if new Pod is scheduled to any of the three zones, it will violate MaxSkew. This is a beta field and requires the MinDomainsInPodTopologySpread feature gate to be enabled (enabled by default). Format : int32 false nodeAffinityPolicy string NodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew. Options are: - Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations. - Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations. If this value is nil, the behavior is equivalent to the Honor policy. This is a alpha-level feature enabled by the NodeInclusionPolicyInPodTopologySpread feature flag. false nodeTaintsPolicy string NodeTaintsPolicy indicates how we will treat node taints when calculating pod topology spread skew. Options are: - Honor: nodes without taints, along with tainted nodes for which the incoming pod has a toleration, are included. - Ignore: node taints are ignored. All nodes are included. If this value is nil, the behavior is equivalent to the Ignore policy. This is a alpha-level feature enabled by the NodeInclusionPolicyInPodTopologySpread feature flag. false","title":"TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index]"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymenttopologyspreadconstraintsindexlabelselector","text":"LabelSelector is used to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain. Name Type Description Required matchExpressions []object matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false","title":"TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index].labelSelector"},{"location":"reference/api/#tenantcontrolplanespeccontrolplanedeploymenttopologyspreadconstraintsindexlabelselectormatchexpressionsindex","text":"A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values. Name Type Description Required key string key is the label key that the selector applies to. true operator string operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false","title":"TenantControlPlane.spec.controlPlane.deployment.topologySpreadConstraints[index].labelSelector.matchExpressions[index]"},{"location":"reference/api/#tenantcontrolplanespeccontrolplaneingress","text":"Defining the options for an Optional Ingress which will expose API Server of the Tenant Control Plane Name Type Description Required additionalMetadata object AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. false hostname string Hostname is an optional field which will be used as Ingress's Host. If it is not defined, Ingress's host will be \" . . \", where domain is specified under NetworkProfileSpec false ingressClassName string false","title":"TenantControlPlane.spec.controlPlane.ingress"},{"location":"reference/api/#tenantcontrolplanespeccontrolplaneingressadditionalmetadata","text":"AdditionalMetadata defines which additional metadata, such as labels and annotations, must be attached to the created resource. Name Type Description Required annotations map[string]string false labels map[string]string false","title":"TenantControlPlane.spec.controlPlane.ingress.additionalMetadata"},{"location":"reference/api/#tenantcontrolplanespeckubernetes","text":"Kubernetes specification for tenant control plane Name Type Description Required kubelet object true version string Kubernetes Version for the tenant control plane true admissionControllers []enum List of enabled Admission Controllers for the Tenant cluster. Full reference available here: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers Default : [CertificateApproval CertificateSigning CertificateSubjectRestriction DefaultIngressClass DefaultStorageClass DefaultTolerationSeconds LimitRanger MutatingAdmissionWebhook NamespaceLifecycle PersistentVolumeClaimResize Priority ResourceQuota RuntimeClass ServiceAccount StorageObjectInUseProtection TaintNodesByCondition ValidatingAdmissionWebhook] false","title":"TenantControlPlane.spec.kubernetes"},{"location":"reference/api/#tenantcontrolplanespeckuberneteskubelet","text":"Name Type Description Required cgroupfs enum CGroupFS defines the cgroup driver for Kubelet https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/ Enum : systemd, cgroupfs false","title":"TenantControlPlane.spec.kubernetes.kubelet"},{"location":"reference/api/#tenantcontrolplanespecaddons","text":"Addons contain which addons are enabled Name Type Description Required coreDNS object Enables the DNS addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to `coredns`. false konnectivity object Enables the Konnectivity addon in the Tenant Cluster, required if the worker nodes are in a different network. false kubeProxy object Enables the kube-proxy addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to `kube-proxy`. false","title":"TenantControlPlane.spec.addons"},{"location":"reference/api/#tenantcontrolplanespecaddonscoredns","text":"Enables the DNS addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to coredns . Name Type Description Required imageRepository string ImageRepository sets the container registry to pull images from. if not set, the default ImageRepository will be used instead. false imageTag string ImageTag allows to specify a tag for the image. In case this value is set, kubeadm does not change automatically the version of the above components during upgrades. false","title":"TenantControlPlane.spec.addons.coreDNS"},{"location":"reference/api/#tenantcontrolplanespecaddonskonnectivity","text":"Enables the Konnectivity addon in the Tenant Cluster, required if the worker nodes are in a different network. Name Type Description Required proxyPort integer Port of Konnectivity proxy server. Format : int32 true agentImage string AgentImage defines the container image for Konnectivity's agent. Default : registry.k8s.io/kas-network-proxy/proxy-agent false resources object Resources define the amount of CPU and memory to allocate to the Konnectivity server. false serverImage string ServerImage defines the container image for Konnectivity's server. Default : registry.k8s.io/kas-network-proxy/proxy-server false version string Version for Konnectivity server and agent. Default : v0.0.32 false","title":"TenantControlPlane.spec.addons.konnectivity"},{"location":"reference/api/#tenantcontrolplanespecaddonskonnectivityresources","text":"Resources define the amount of CPU and memory to allocate to the Konnectivity server. Name Type Description Required limits map[string]int or string Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false","title":"TenantControlPlane.spec.addons.konnectivity.resources"},{"location":"reference/api/#tenantcontrolplanespecaddonskubeproxy","text":"Enables the kube-proxy addon in the Tenant Cluster. The registry and the tag are configurable, the image is hard-coded to kube-proxy . Name Type Description Required imageRepository string ImageRepository sets the container registry to pull images from. if not set, the default ImageRepository will be used instead. false imageTag string ImageTag allows to specify a tag for the image. In case this value is set, kubeadm does not change automatically the version of the above components during upgrades. false","title":"TenantControlPlane.spec.addons.kubeProxy"},{"location":"reference/api/#tenantcontrolplanespecnetworkprofile","text":"NetworkProfile specifies how the network is Name Type Description Required address string Address where API server of will be exposed. In case of LoadBalancer Service, this can be empty in order to use the exposed IP provided by the cloud controller manager. false allowAddressAsExternalIP boolean AllowAddressAsExternalIP will include tenantControlPlane.Spec.NetworkProfile.Address in the section of ExternalIPs of the Kubernetes Service (only ClusterIP or NodePort) false certSANs []string CertSANs sets extra Subject Alternative Names (SANs) for the API Server signing certificate. Use this field to add additional hostnames when exposing the Tenant Control Plane with third solutions. false dnsServiceIPs []string Default : [10.96.0.10] false podCidr string CIDR for Kubernetes Pods Default : 10.244.0.0/16 false port integer Port where API server of will be exposed Format : int32 Default : 6443 false serviceCidr string Kubernetes Service Default : 10.96.0.0/16 false","title":"TenantControlPlane.spec.networkProfile"},{"location":"reference/api/#tenantcontrolplanestatus","text":"TenantControlPlaneStatus defines the observed state of TenantControlPlane. Name Type Description Required addons object Addons contains the status of the different Addons false certificates object Certificates contains information about the different certificates that are necessary to run a kubernetes control plane false controlPlaneEndpoint string ControlPlaneEndpoint contains the status of the kubernetes control plane false kubeadmPhase object KubeadmPhase contains the status of the kubeadm phases action false kubeadmconfig object KubeadmConfig contains the status of the configuration required by kubeadm false kubeconfig object KubeConfig contains information about the kubenconfigs that control plane pieces need false kubernetesResources object Kubernetes contains information about the reconciliation of the required Kubernetes resources deployed in the admin cluster false storage object Storage Status contains information about Kubernetes storage system false","title":"TenantControlPlane.status"},{"location":"reference/api/#tenantcontrolplanestatusaddons","text":"Addons contains the status of the different Addons Name Type Description Required coreDNS object AddonStatus defines the observed state of an Addon. false konnectivity object KonnectivityStatus defines the status of Konnectivity as Addon. false kubeProxy object AddonStatus defines the observed state of an Addon. false","title":"TenantControlPlane.status.addons"},{"location":"reference/api/#tenantcontrolplanestatusaddonscoredns","text":"AddonStatus defines the observed state of an Addon. Name Type Description Required enabled boolean true checksum string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.addons.coreDNS"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivity","text":"KonnectivityStatus defines the status of Konnectivity as Addon. Name Type Description Required enabled boolean true agent object false certificate object CertificatePrivateKeyPairStatus defines the status. false clusterrolebinding object false configMap object false kubeconfig object KubeconfigStatus contains information about the generated kubeconfig. false sa object false service object KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. false","title":"TenantControlPlane.status.addons.konnectivity"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityagent","text":"Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false","title":"TenantControlPlane.status.addons.konnectivity.agent"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivitycertificate","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.addons.konnectivity.certificate"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityclusterrolebinding","text":"Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false","title":"TenantControlPlane.status.addons.konnectivity.clusterrolebinding"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityconfigmap","text":"Name Type Description Required checksum string false name string false","title":"TenantControlPlane.status.addons.konnectivity.configMap"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivitykubeconfig","text":"KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.addons.konnectivity.kubeconfig"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivitysa","text":"Name Type Description Required checksum string false lastUpdate string Last time when k8s object was updated Format : date-time false name string false namespace string false","title":"TenantControlPlane.status.addons.konnectivity.sa"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityservice","text":"KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. Name Type Description Required name string The name of the Service for the given cluster. true namespace string The namespace which the Service for the given cluster is deployed. true port integer The port where the service is running Format : int32 true conditions []object Current service state false loadBalancer object LoadBalancer contains the current status of the load-balancer, if one is present. false","title":"TenantControlPlane.status.addons.konnectivity.service"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityserviceconditionsindex","text":"Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions. For example, type FooStatus struct{ // Represents the observations of a foo's current state. // Known .status.conditions.type are: \"Available\", \"Progressing\", and \"Degraded\" // +patchMergeKey=type // +patchStrategy=merge // +listType=map // +listMapKey=type Conditions []metav1.Condition json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,1,rep,name=conditions\" // other fields } Name Type Description Required lastTransitionTime string lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format : date-time true message string message is a human readable message indicating details about the transition. This may be an empty string. true reason string reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum status of the condition, one of True, False, Unknown. Enum : True, False, Unknown true type string type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) true observedGeneration integer observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format : int64 Minimum : 0 false","title":"TenantControlPlane.status.addons.konnectivity.service.conditions[index]"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityserviceloadbalancer","text":"LoadBalancer contains the current status of the load-balancer, if one is present. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false","title":"TenantControlPlane.status.addons.konnectivity.service.loadBalancer"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityserviceloadbalanceringressindex","text":"LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false","title":"TenantControlPlane.status.addons.konnectivity.service.loadBalancer.ingress[index]"},{"location":"reference/api/#tenantcontrolplanestatusaddonskonnectivityserviceloadbalanceringressindexportsindex","text":"Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false","title":"TenantControlPlane.status.addons.konnectivity.service.loadBalancer.ingress[index].ports[index]"},{"location":"reference/api/#tenantcontrolplanestatusaddonskubeproxy","text":"AddonStatus defines the observed state of an Addon. Name Type Description Required enabled boolean true checksum string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.addons.kubeProxy"},{"location":"reference/api/#tenantcontrolplanestatuscertificates","text":"Certificates contains information about the different certificates that are necessary to run a kubernetes control plane Name Type Description Required apiServer object CertificatePrivateKeyPairStatus defines the status. false apiServerKubeletClient object CertificatePrivateKeyPairStatus defines the status. false ca object CertificatePrivateKeyPairStatus defines the status. false etcd object ETCDCertificatesStatus defines the observed state of ETCD Certificate for API server. false frontProxyCA object CertificatePrivateKeyPairStatus defines the status. false frontProxyClient object CertificatePrivateKeyPairStatus defines the status. false sa object PublicKeyPrivateKeyPairStatus defines the status. false","title":"TenantControlPlane.status.certificates"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesapiserver","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.apiServer"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesapiserverkubeletclient","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.apiServerKubeletClient"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesca","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.ca"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesetcd","text":"ETCDCertificatesStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required apiServer object APIServerCertificatesStatus defines the observed state of ETCD Certificate for API server. false ca object ETCDCertificateStatus defines the observed state of ETCD Certificate for API server. false","title":"TenantControlPlane.status.certificates.etcd"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesetcdapiserver","text":"APIServerCertificatesStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.etcd.apiServer"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesetcdca","text":"ETCDCertificateStatus defines the observed state of ETCD Certificate for API server. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.etcd.ca"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesfrontproxyca","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.frontProxyCA"},{"location":"reference/api/#tenantcontrolplanestatuscertificatesfrontproxyclient","text":"CertificatePrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.frontProxyClient"},{"location":"reference/api/#tenantcontrolplanestatuscertificatessa","text":"PublicKeyPrivateKeyPairStatus defines the status. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.certificates.sa"},{"location":"reference/api/#tenantcontrolplanestatuskubeadmphase","text":"KubeadmPhase contains the status of the kubeadm phases action Name Type Description Required bootstrapToken object KubeadmPhaseStatus contains the status of a kubeadm phase action. true uploadConfigKubeadm object KubeadmPhaseStatus contains the status of a kubeadm phase action. true uploadConfigKubelet object KubeadmPhaseStatus contains the status of a kubeadm phase action. true","title":"TenantControlPlane.status.kubeadmPhase"},{"location":"reference/api/#tenantcontrolplanestatuskubeadmphasebootstraptoken","text":"KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.kubeadmPhase.bootstrapToken"},{"location":"reference/api/#tenantcontrolplanestatuskubeadmphaseuploadconfigkubeadm","text":"KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.kubeadmPhase.uploadConfigKubeadm"},{"location":"reference/api/#tenantcontrolplanestatuskubeadmphaseuploadconfigkubelet","text":"KubeadmPhaseStatus contains the status of a kubeadm phase action. Name Type Description Required checksum string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.kubeadmPhase.uploadConfigKubelet"},{"location":"reference/api/#tenantcontrolplanestatuskubeadmconfig","text":"KubeadmConfig contains the status of the configuration required by kubeadm Name Type Description Required checksum string Checksum of the kubeadm configuration to detect changes false configmapName string false lastUpdate string Format : date-time false","title":"TenantControlPlane.status.kubeadmconfig"},{"location":"reference/api/#tenantcontrolplanestatuskubeconfig","text":"KubeConfig contains information about the kubenconfigs that control plane pieces need Name Type Description Required admin object KubeconfigStatus contains information about the generated kubeconfig. false controllerManager object KubeconfigStatus contains information about the generated kubeconfig. false scheduler object KubeconfigStatus contains information about the generated kubeconfig. false","title":"TenantControlPlane.status.kubeconfig"},{"location":"reference/api/#tenantcontrolplanestatuskubeconfigadmin","text":"KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.kubeconfig.admin"},{"location":"reference/api/#tenantcontrolplanestatuskubeconfigcontrollermanager","text":"KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.kubeconfig.controllerManager"},{"location":"reference/api/#tenantcontrolplanestatuskubeconfigscheduler","text":"KubeconfigStatus contains information about the generated kubeconfig. Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.kubeconfig.scheduler"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresources","text":"Kubernetes contains information about the reconciliation of the required Kubernetes resources deployed in the admin cluster Name Type Description Required deployment object KubernetesDeploymentStatus defines the status for the Tenant Control Plane Deployment in the management cluster. false ingress object KubernetesIngressStatus defines the status for the Tenant Control Plane Ingress in the management cluster. false service object KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. false version object KubernetesVersion contains the information regarding the running Kubernetes version, and its upgrade status. false","title":"TenantControlPlane.status.kubernetesResources"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesdeployment","text":"KubernetesDeploymentStatus defines the status for the Tenant Control Plane Deployment in the management cluster. Name Type Description Required name string The name of the Deployment for the given cluster. true namespace string The namespace which the Deployment for the given cluster is deployed. true selector string Selector is the label selector used to group the Tenant Control Plane Pods used by the scale subresource. true availableReplicas integer Total number of available pods (ready for at least minReadySeconds) targeted by this deployment. Format : int32 false collisionCount integer Count of hash collisions for the Deployment. The Deployment controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ReplicaSet. Format : int32 false conditions []object Represents the latest available observations of a deployment's current state. false lastUpdate string Last time when deployment was updated Format : date-time false observedGeneration integer The generation observed by the deployment controller. Format : int64 false readyReplicas integer readyReplicas is the number of pods targeted by this Deployment with a Ready Condition. Format : int32 false replicas integer Total number of non-terminated pods targeted by this deployment (their labels match the selector). Format : int32 false unavailableReplicas integer Total number of unavailable pods targeted by this deployment. This is the total number of pods that are still required for the deployment to have 100% available capacity. They may either be pods that are running but not yet available or pods that still have not been created. Format : int32 false updatedReplicas integer Total number of non-terminated pods targeted by this deployment that have the desired template spec. Format : int32 false","title":"TenantControlPlane.status.kubernetesResources.deployment"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesdeploymentconditionsindex","text":"DeploymentCondition describes the state of a deployment at a certain point. Name Type Description Required status string Status of the condition, one of True, False, Unknown. true type string Type of deployment condition. true lastTransitionTime string Last time the condition transitioned from one status to another. Format : date-time false lastUpdateTime string The last time this condition was updated. Format : date-time false message string A human readable message indicating details about the transition. false reason string The reason for the condition's last transition. false","title":"TenantControlPlane.status.kubernetesResources.deployment.conditions[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesingress","text":"KubernetesIngressStatus defines the status for the Tenant Control Plane Ingress in the management cluster. Name Type Description Required name string The name of the Ingress for the given cluster. true namespace string The namespace which the Ingress for the given cluster is deployed. true loadBalancer object LoadBalancer contains the current status of the load-balancer. false","title":"TenantControlPlane.status.kubernetesResources.ingress"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesingressloadbalancer","text":"LoadBalancer contains the current status of the load-balancer. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false","title":"TenantControlPlane.status.kubernetesResources.ingress.loadBalancer"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesingressloadbalanceringressindex","text":"LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false","title":"TenantControlPlane.status.kubernetesResources.ingress.loadBalancer.ingress[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesingressloadbalanceringressindexportsindex","text":"Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false","title":"TenantControlPlane.status.kubernetesResources.ingress.loadBalancer.ingress[index].ports[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesservice","text":"KubernetesServiceStatus defines the status for the Tenant Control Plane Service in the management cluster. Name Type Description Required name string The name of the Service for the given cluster. true namespace string The namespace which the Service for the given cluster is deployed. true port integer The port where the service is running Format : int32 true conditions []object Current service state false loadBalancer object LoadBalancer contains the current status of the load-balancer, if one is present. false","title":"TenantControlPlane.status.kubernetesResources.service"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesserviceconditionsindex","text":"Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions. For example, type FooStatus struct{ // Represents the observations of a foo's current state. // Known .status.conditions.type are: \"Available\", \"Progressing\", and \"Degraded\" // +patchMergeKey=type // +patchStrategy=merge // +listType=map // +listMapKey=type Conditions []metav1.Condition json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,1,rep,name=conditions\" // other fields } Name Type Description Required lastTransitionTime string lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format : date-time true message string message is a human readable message indicating details about the transition. This may be an empty string. true reason string reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status enum status of the condition, one of True, False, Unknown. Enum : True, False, Unknown true type string type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) true observedGeneration integer observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format : int64 Minimum : 0 false","title":"TenantControlPlane.status.kubernetesResources.service.conditions[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesserviceloadbalancer","text":"LoadBalancer contains the current status of the load-balancer, if one is present. Name Type Description Required ingress []object Ingress is a list containing ingress points for the load-balancer. Traffic intended for the service should be sent to these ingress points. false","title":"TenantControlPlane.status.kubernetesResources.service.loadBalancer"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesserviceloadbalanceringressindex","text":"LoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point. Name Type Description Required hostname string Hostname is set for load-balancer ingress points that are DNS based (typically AWS load-balancers) false ip string IP is set for load-balancer ingress points that are IP based (typically GCE or OpenStack load-balancers) false ports []object Ports is a list of records of service ports If used, every port defined in the service should have an entry in it false","title":"TenantControlPlane.status.kubernetesResources.service.loadBalancer.ingress[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesserviceloadbalanceringressindexportsindex","text":"Name Type Description Required port integer Port is the port number of the service port of which status is recorded here Format : int32 true protocol string Protocol is the protocol of the service port of which status is recorded here The supported values are: \"TCP\", \"UDP\", \"SCTP\" Default : TCP true error string Error is to record the problem with the service port The format of the error shall comply with the following rules: - built-in error values shall be specified in this file and those shall use CamelCase names - cloud provider specific error values must have names that comply with the format foo.example.com/CamelCase. --- The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) false","title":"TenantControlPlane.status.kubernetesResources.service.loadBalancer.ingress[index].ports[index]"},{"location":"reference/api/#tenantcontrolplanestatuskubernetesresourcesversion","text":"KubernetesVersion contains the information regarding the running Kubernetes version, and its upgrade status. Name Type Description Required status enum Status returns the current status of the Kubernetes version, such as its provisioning state, or completed upgrade. Enum : Provisioning, Upgrading, Ready, NotReady Default : Provisioning false version string Version is the running Kubernetes version of the Tenant Control Plane. false","title":"TenantControlPlane.status.kubernetesResources.version"},{"location":"reference/api/#tenantcontrolplanestatusstorage","text":"Storage Status contains information about Kubernetes storage system Name Type Description Required certificate object false config object false dataStoreName string false driver string false setup object false","title":"TenantControlPlane.status.storage"},{"location":"reference/api/#tenantcontrolplanestatusstoragecertificate","text":"Name Type Description Required checksum string false lastUpdate string Format : date-time false secretName string false","title":"TenantControlPlane.status.storage.certificate"},{"location":"reference/api/#tenantcontrolplanestatusstorageconfig","text":"Name Type Description Required checksum string false secretName string false","title":"TenantControlPlane.status.storage.config"},{"location":"reference/api/#tenantcontrolplanestatusstoragesetup","text":"Name Type Description Required checksum string false lastUpdate string Format : date-time false schema string false user string false","title":"TenantControlPlane.status.storage.setup"},{"location":"reference/configuration/","text":"Configuration Currently Kamaji supports (in this order): CLI flags Environment variables Configuration files By default Kamaji search for the configuration file ( kamaji.yaml ) and uses parameters found inside of it. In case some environment variable are passed, this will override configuration file parameters. In the end, if also a CLI flag is passed, this will override both env vars and config file as well. This is easily explained in this way: cli-flags > env-vars > config-files Available flags are the following: --config-file string Configuration file alternative. (default \"kamaji.yaml\") --datastore string The default DataStore that should be used by Kamaji to setup the required storage (default \"etcd\") --health-probe-bind-address string The address the probe endpoint binds to. (default \":8081\") --kine-image string Container image along with tag to use for the Kine sidecar container (used only if etcd-storage-type is set to one of kine strategies) (default \"rancher/kine:v0.9.2-amd64\") --kubeconfig string Paths to a kubeconfig. Only required if out-of-cluster. --leader-elect Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager. --metrics-bind-address string The address the metric endpoint binds to. (default \":8080\") --tmp-directory string Directory which will be used to work with temporary files. (default \"/tmp/kamaji\") --zap-devel Development Mode defaults(encoder=consoleEncoder,logLevel=Debug,stackTraceLevel=Warn). Production Mode defaults(encoder=jsonEncoder,logLevel=Info,stackTraceLevel=Error) (default true) --zap-encoder encoder Zap log encoding (one of 'json' or 'console') --zap-log-level level Zap Level to configure the verbosity of logging. Can be one of 'debug', 'info', 'error', or any integer value > 0 which corresponds to custom debug levels of increasing verbosity --zap-stacktrace-level level Zap Level at and above which stacktraces are captured (one of 'info', 'error', 'panic'). --zap-time-encoding time-encoding Zap time encoding (one of 'epoch', 'millis', 'nano', 'iso8601', 'rfc3339' or 'rfc3339nano'). Defaults to 'epoch'. Available environment variables are: Environment variable Description KAMAJI_DATASTORE Name of the DataStore resource with driver definition and settings. (default \"etcd\") KAMAJI_METRICS_BIND_ADDRESS The address the metric endpoint binds to. (default \":8080\") KAMAJI_HEALTH_PROBE_BIND_ADDRESS The address the probe endpoint binds to. (default \":8081\") KAMAJI_LEADER_ELECTION Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager. KAMAJI_TMP_DIRECTORY Directory which will be used to work with temporary files. (default \"/tmp/kamaji\")","title":"Configuration"},{"location":"reference/configuration/#configuration","text":"Currently Kamaji supports (in this order): CLI flags Environment variables Configuration files By default Kamaji search for the configuration file ( kamaji.yaml ) and uses parameters found inside of it. In case some environment variable are passed, this will override configuration file parameters. In the end, if also a CLI flag is passed, this will override both env vars and config file as well. This is easily explained in this way: cli-flags > env-vars > config-files Available flags are the following: --config-file string Configuration file alternative. (default \"kamaji.yaml\") --datastore string The default DataStore that should be used by Kamaji to setup the required storage (default \"etcd\") --health-probe-bind-address string The address the probe endpoint binds to. (default \":8081\") --kine-image string Container image along with tag to use for the Kine sidecar container (used only if etcd-storage-type is set to one of kine strategies) (default \"rancher/kine:v0.9.2-amd64\") --kubeconfig string Paths to a kubeconfig. Only required if out-of-cluster. --leader-elect Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager. --metrics-bind-address string The address the metric endpoint binds to. (default \":8080\") --tmp-directory string Directory which will be used to work with temporary files. (default \"/tmp/kamaji\") --zap-devel Development Mode defaults(encoder=consoleEncoder,logLevel=Debug,stackTraceLevel=Warn). Production Mode defaults(encoder=jsonEncoder,logLevel=Info,stackTraceLevel=Error) (default true) --zap-encoder encoder Zap log encoding (one of 'json' or 'console') --zap-log-level level Zap Level to configure the verbosity of logging. Can be one of 'debug', 'info', 'error', or any integer value > 0 which corresponds to custom debug levels of increasing verbosity --zap-stacktrace-level level Zap Level at and above which stacktraces are captured (one of 'info', 'error', 'panic'). --zap-time-encoding time-encoding Zap time encoding (one of 'epoch', 'millis', 'nano', 'iso8601', 'rfc3339' or 'rfc3339nano'). Defaults to 'epoch'. Available environment variables are: Environment variable Description KAMAJI_DATASTORE Name of the DataStore resource with driver definition and settings. (default \"etcd\") KAMAJI_METRICS_BIND_ADDRESS The address the metric endpoint binds to. (default \":8080\") KAMAJI_HEALTH_PROBE_BIND_ADDRESS The address the probe endpoint binds to. (default \":8081\") KAMAJI_LEADER_ELECTION Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager. KAMAJI_TMP_DIRECTORY Directory which will be used to work with temporary files. (default \"/tmp/kamaji\")","title":"Configuration"},{"location":"reference/conformance/","text":"Conformance For organizations using Kubernetes, conformance enables interoperability, consistency, and confirmability between Kubernetes installations. The Cloud Computing Native Foundation - CNCF - provides the Certified Kubernetes Conformance Program . The standard set of conformance tests is currently those defined by the [Conformance] tag in the kubernetes e2e suite. All the \u201ctenant clusters\u201d built with Kamaji are CNCF conformant: v1.23 v1.24 v1.25 Running the conformance tests The standard tool for running CNCF conformance tests is Sonobuoy . Sonobuoy is regularly built and kept up to date to execute against all currently supported versions of kubernetes. Download a binary release of the CLI. Make sure to access your tenant cluster: export KUBECONFIG=tenant.kubeconfig Deploy a Sonobuoy pod to your tenant cluster with: sonobuoy run --mode=certified-conformance You can run the command synchronously by adding the flag --wait but be aware that running the conformance tests can take an hour or more. View actively running pods: sonobuoy status To inspect the logs: sonobuoy logs -f Once sonobuoy status shows the run as completed , copy the output directory from the main Sonobuoy pod to a local directory: outfile=$(sonobuoy retrieve) This copies a single .tar.gz snapshot from the Sonobuoy pod into your local . directory. Extract the contents into ./results with: mkdir ./results; tar xzf $outfile -C ./results To clean up Kubernetes objects created by Sonobuoy, run: sonobuoy delete","title":"Conformance"},{"location":"reference/conformance/#conformance","text":"For organizations using Kubernetes, conformance enables interoperability, consistency, and confirmability between Kubernetes installations. The Cloud Computing Native Foundation - CNCF - provides the Certified Kubernetes Conformance Program . The standard set of conformance tests is currently those defined by the [Conformance] tag in the kubernetes e2e suite. All the \u201ctenant clusters\u201d built with Kamaji are CNCF conformant: v1.23 v1.24 v1.25","title":"Conformance"},{"location":"reference/conformance/#running-the-conformance-tests","text":"The standard tool for running CNCF conformance tests is Sonobuoy . Sonobuoy is regularly built and kept up to date to execute against all currently supported versions of kubernetes. Download a binary release of the CLI. Make sure to access your tenant cluster: export KUBECONFIG=tenant.kubeconfig Deploy a Sonobuoy pod to your tenant cluster with: sonobuoy run --mode=certified-conformance You can run the command synchronously by adding the flag --wait but be aware that running the conformance tests can take an hour or more. View actively running pods: sonobuoy status To inspect the logs: sonobuoy logs -f Once sonobuoy status shows the run as completed , copy the output directory from the main Sonobuoy pod to a local directory: outfile=$(sonobuoy retrieve) This copies a single .tar.gz snapshot from the Sonobuoy pod into your local . directory. Extract the contents into ./results with: mkdir ./results; tar xzf $outfile -C ./results To clean up Kubernetes objects created by Sonobuoy, run: sonobuoy delete","title":"Running the conformance tests"},{"location":"reference/versioning/","text":"Versioning In Kamaji, there are different components that might require independent versioning and support level: Kamaji Admin Cluster Tenant Cluster (min) Tenant Cluster (max) Konnectivity Tenant etcd 0.0.1 1.22.0+ 1.21.0 1.23.5 0.0.31 3.5.4 0.0.2 1.22.0+ 1.21.0 1.25.0 0.0.32 3.5.4 Other combinations might work but they have not been yet tested.","title":"Versioning"},{"location":"reference/versioning/#versioning","text":"In Kamaji, there are different components that might require independent versioning and support level: Kamaji Admin Cluster Tenant Cluster (min) Tenant Cluster (max) Konnectivity Tenant etcd 0.0.1 1.22.0+ 1.21.0 1.23.5 0.0.31 3.5.4 0.0.2 1.22.0+ 1.21.0 1.25.0 0.0.32 3.5.4 Other combinations might work but they have not been yet tested.","title":"Versioning"}]}